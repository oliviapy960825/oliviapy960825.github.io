{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Coefficients: \n",
      " [938.23786125]\n",
      "Mean squared error: 2548.07\n",
      "Variance score: 0.47\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Code source: Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
    "\n",
    "#print(diabetes_X)\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test = diabetes.target[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "(442, 10)\n",
      "(442,)\n",
      "Coefficients: \n",
      " [ 3.03499549e-01 -2.37639315e+02  5.10530605e+02  3.27736980e+02\n",
      " -8.14131709e+02  4.92814588e+02  1.02848452e+02  1.84606489e+02\n",
      "  7.43519617e+02  7.60951722e+01]\n",
      "Mean squared error: 2004.57\n",
      "Variance score: 0.59\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Code source: Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "#print(diabetes.data)\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes.data\n",
    "print(diabetes.data.shape)\n",
    "print(diabetes.target.shape)\n",
    "#print(diabetes_X)\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test = diabetes.target[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "KFold(n_splits=10, random_state=<mtrand.RandomState object at 0x1a20f59fc0>,\n",
      "   shuffle=True)\n",
      "TRAIN: [  0   2   3   4   5   6   7   8   9  11  13  14  15  16  17  18  19  20\n",
      "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  38  39  40\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  77\n",
      "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
      "  97  98  99 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115\n",
      " 116 117 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134\n",
      " 135 136 137 138 139 140 141 142 143 145 146 147 148 149 150 151 152 153\n",
      " 154 155 156 157 159 160 161 162 163 164 165 166 167 168 169 170 172 173\n",
      " 174 175 176 177 178 180 181 182 183 184 185 186 187 188 189 190 191 192\n",
      " 193 195 196 197 199 200 201 202 203 204 207 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 239 240 241 242 243 244 245 246 247 248 250 251 252 253\n",
      " 254 255 256 257 258 259 260 262 263 264 265 266 267 269 270 272 273 274\n",
      " 275 276 277 278 279 280 281 282 284 285 286 288 290 291 292 293 294 295\n",
      " 296 297 299 300 301 303 304 305 306 307 308 309 310 311 312 313 314 315\n",
      " 316 317 318 320 321 322 323 324 325 326 327 328 329 331 332 333 334 335\n",
      " 336 337 338 340 341 342 343 345 346 347 348 349 350 351 352 353 354 355\n",
      " 356 357 358 359 361 363 364 365 366 367 368 369 370 371 372 374 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 389 390 391 392 393 394 395 396\n",
      " 398 402 404 405 406 407 408 409 410 412 413 414 415 416 417 418 419 420\n",
      " 421 422 423 424 425 426 427 428 429 430 431 432 433 436 437 438 439 440\n",
      " 441] TEST: [  1  10  12  21  37  76  78 100 118 144 158 171 179 194 198 205 206 208\n",
      " 238 249 261 268 271 283 287 289 298 302 319 330 339 344 360 362 373 375\n",
      " 388 397 399 400 401 403 411 434 435]\n",
      "Coefficients : \n",
      " [ -38.78692116 -236.14171933  529.50614018  329.3901085  -653.71944462\n",
      "  384.05013757   49.11947973  139.13384674  718.0280958    75.34267153]\n",
      "Mean squared error is: 3111.99\n",
      "Variance score is: 0.36\n",
      "\n",
      "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  16  17  18  19\n",
      "  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37\n",
      "  38  39  40  41  42  43  44  45  46  47  48  50  51  53  55  57  58  61\n",
      "  62  63  64  66  67  68  69  70  72  73  75  76  77  78  79  80  81  82\n",
      "  83  84  85  86  87  88  89  91  92  93  94  95  97  98  99 100 101 103\n",
      " 104 105 106 108 109 110 111 112 114 115 116 117 118 119 120 121 123 125\n",
      " 126 127 128 129 130 131 133 135 136 137 138 139 140 143 144 145 146 147\n",
      " 148 149 150 151 152 153 154 156 158 161 162 163 165 166 167 168 169 171\n",
      " 172 173 174 175 176 177 178 179 180 181 182 183 184 185 187 189 190 191\n",
      " 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209\n",
      " 210 211 212 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228\n",
      " 229 230 231 232 234 235 236 237 238 239 240 241 242 243 244 245 246 247\n",
      " 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265\n",
      " 266 267 268 269 270 271 272 273 274 275 277 278 279 280 281 283 285 286\n",
      " 287 288 289 290 291 292 293 294 295 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 321 322 323 324\n",
      " 325 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 344\n",
      " 345 346 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363\n",
      " 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 383\n",
      " 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401\n",
      " 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419\n",
      " 420 421 422 423 424 425 426 428 429 430 431 432 433 434 435 436 437 439\n",
      " 440] TEST: [  6  15  49  52  54  56  59  60  65  71  74  90  96 102 107 113 122 124\n",
      " 132 134 141 142 155 157 159 160 164 170 186 188 213 233 276 282 284 296\n",
      " 320 326 343 347 381 382 427 438 441]\n",
      "Coefficients : \n",
      " [  -5.66645646 -241.41387424  547.72127256  303.81957877 -809.08480973\n",
      "  431.74384559   85.87668545  202.15785848  767.79017188   43.7483751 ]\n",
      "Mean squared error is: 3767.00\n",
      "Variance score is: 0.27\n",
      "\n",
      "TRAIN: [  0   1   2   3   6   9  10  11  12  13  15  16  17  18  19  21  23  24\n",
      "  25  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43\n",
      "  44  46  47  48  49  50  51  52  53  54  56  57  58  59  60  61  62  63\n",
      "  65  66  67  69  70  71  72  73  74  76  77  78  79  80  82  83  84  85\n",
      "  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103\n",
      " 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121\n",
      " 122 123 124 125 126 127 128 129 130 131 132 133 134 136 137 138 139 140\n",
      " 141 142 143 144 145 146 147 148 149 150 151 152 155 156 157 158 159 160\n",
      " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 191 192 193 194 195 196 197 198\n",
      " 199 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217\n",
      " 218 219 220 221 222 223 224 226 227 228 229 230 231 233 234 235 236 237\n",
      " 238 239 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256\n",
      " 257 258 259 260 261 262 263 265 266 267 268 269 270 271 272 273 274 275\n",
      " 276 277 278 279 280 282 283 284 285 286 287 288 289 290 291 292 293 294\n",
      " 295 296 297 298 299 301 302 303 304 305 306 307 308 309 310 311 312 314\n",
      " 315 317 318 319 320 321 322 323 324 326 330 331 332 333 334 335 336 337\n",
      " 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 354 355 356\n",
      " 357 358 359 360 362 363 364 365 367 368 370 372 373 374 375 376 377 378\n",
      " 379 380 381 382 383 384 385 387 388 390 391 392 393 394 395 396 397 398\n",
      " 399 400 401 402 403 404 406 407 408 409 410 411 412 413 416 417 418 419\n",
      " 420 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 438 439\n",
      " 440 441] TEST: [  4   5   7   8  14  20  22  26  45  55  64  68  75  81 135 153 154 175\n",
      " 190 200 225 232 240 264 281 300 313 316 325 327 328 329 353 361 366 369\n",
      " 371 386 389 405 414 415 421 437]\n",
      "Coefficients : \n",
      " [ -19.46261078 -209.06810099  531.03072749  307.2177886  -651.02990001\n",
      "  390.45644812   28.97163256  125.94108105  725.71900736   48.44961362]\n",
      "Mean squared error is: 2346.30\n",
      "Variance score is: 0.55\n",
      "\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  19\n",
      "  20  21  22  23  24  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
      "  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56\n",
      "  57  58  59  60  61  62  64  65  67  68  69  70  71  72  73  74  75  76\n",
      "  77  78  79  80  81  82  83  84  85  86  87  88  90  91  94  95  96  97\n",
      "  98  99 100 102 104 105 107 108 109 110 111 112 113 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 141 142 143 144 147 148 149 151 152 153 154 155 156 157 158 159\n",
      " 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 177 178\n",
      " 179 180 181 182 183 184 185 186 187 188 189 190 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 221 222 223 225 226 227 228 229 230 232 233 234 236 237 238\n",
      " 239 240 242 243 244 246 248 249 250 251 253 255 256 257 258 259 260 261\n",
      " 262 263 264 265 266 267 268 269 270 271 273 275 276 277 279 280 281 282\n",
      " 283 284 285 286 287 288 289 290 291 292 293 294 295 296 298 299 300 301\n",
      " 302 303 304 305 306 307 308 309 310 311 312 313 314 316 317 318 319 320\n",
      " 321 323 324 325 326 327 328 329 330 331 332 333 334 335 337 338 339 340\n",
      " 341 342 343 344 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 364 365 366 367 368 369 370 371 372 373 374 375 376 377 379\n",
      " 381 382 383 384 385 386 388 389 390 391 392 394 395 396 397 398 399 400\n",
      " 401 402 403 404 405 406 407 408 409 410 411 413 414 415 416 417 419 420\n",
      " 421 422 423 424 425 426 427 428 429 431 432 433 434 435 436 437 438 439\n",
      " 440 441] TEST: [ 17  18  30  63  66  89  92  93 101 103 106 114 140 145 146 150 176 191\n",
      " 219 220 224 231 235 241 245 247 252 254 272 274 278 297 315 322 336 345\n",
      " 363 378 380 387 393 412 418 430]\n",
      "Coefficients : \n",
      " [ -22.21311839 -264.84046102  495.7797829   377.96091421 -998.16075766\n",
      "  628.09797993  183.74248733  188.07738422  836.49643564   84.9793742 ]\n",
      "Mean squared error is: 3501.05\n",
      "Variance score is: 0.37\n",
      "\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  30  31  32  35  36  37  38\n",
      "  39  40  41  42  43  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  59  60  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
      "  79  80  81  82  83  84  86  87  88  89  90  91  92  93  94  95  96  98\n",
      "  99 100 101 102 103 104 105 106 107 109 110 111 113 114 115 117 118 119\n",
      " 121 122 123 124 125 127 128 129 130 131 132 134 135 138 139 140 141 142\n",
      " 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160\n",
      " 161 163 164 165 166 169 170 171 172 174 175 176 177 178 179 180 182 183\n",
      " 184 185 186 187 188 189 190 191 192 193 194 195 197 198 200 201 202 203\n",
      " 204 205 206 207 208 209 210 211 212 213 214 215 216 217 219 220 222 223\n",
      " 224 225 226 227 228 230 231 232 233 234 235 237 238 239 240 241 242 243\n",
      " 244 245 247 248 249 251 252 254 255 256 257 258 259 260 261 262 264 265\n",
      " 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283\n",
      " 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301\n",
      " 302 304 305 306 307 309 310 311 312 313 314 315 316 317 318 319 320 321\n",
      " 322 323 324 325 326 327 328 329 330 331 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 358 359\n",
      " 360 361 362 363 364 365 366 368 369 370 371 373 374 375 377 378 379 380\n",
      " 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398\n",
      " 399 400 401 402 403 404 405 406 408 409 410 411 412 413 414 415 416 417\n",
      " 418 420 421 423 425 426 427 428 429 430 431 432 434 435 436 437 438 439\n",
      " 440 441] TEST: [ 29  33  34  44  46  61  73  85  97 108 112 116 120 126 133 136 137 162\n",
      " 167 168 173 181 196 199 218 221 229 236 246 250 253 263 303 308 332 357\n",
      " 367 372 376 407 419 422 424 433]\n",
      "Coefficients : \n",
      " [  32.31793253 -263.50962983  512.13625668  302.14377302 -783.77680109\n",
      "  504.00151659  109.13739376  186.76835502  768.24174206   53.72031522]\n",
      "Mean squared error is: 2651.33\n",
      "Variance score is: 0.59\n",
      "\n",
      "TRAIN: [  0   1   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  25  26  28  29  30  31  32  33  34  36  37  38  39\n",
      "  41  42  43  44  45  46  47  48  49  50  52  53  54  55  56  57  58  59\n",
      "  60  61  62  63  64  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
      "  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97\n",
      "  98  99 100 101 102 103 105 106 107 108 109 110 112 113 114 115 116 117\n",
      " 118 119 120 121 122 123 124 125 126 127 128 130 131 132 133 134 135 136\n",
      " 137 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155\n",
      " 157 158 159 160 161 162 163 164 165 167 168 169 170 171 172 173 174 175\n",
      " 176 177 178 179 180 181 182 183 184 185 186 187 188 190 191 192 193 194\n",
      " 195 196 197 198 199 200 201 202 203 205 206 207 208 209 211 213 218 219\n",
      " 220 221 222 224 225 226 227 228 229 231 232 233 235 236 237 238 240 241\n",
      " 242 243 244 245 246 247 248 249 250 251 252 253 254 256 257 258 260 261\n",
      " 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279\n",
      " 280 281 282 283 284 285 286 287 288 289 290 291 292 294 296 297 298 299\n",
      " 300 302 303 304 305 307 308 309 312 313 314 315 316 318 319 320 321 322\n",
      " 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 347 350 351 352 353 354 355 356 357 358 359 360 361\n",
      " 362 363 364 366 367 368 369 370 371 372 373 375 376 377 378 379 380 381\n",
      " 382 383 384 385 386 387 388 389 390 392 393 394 395 396 397 398 399 400\n",
      " 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 418 419\n",
      " 420 421 422 423 424 425 426 427 428 429 430 433 434 435 436 437 438 439\n",
      " 440 441] TEST: [  2  24  27  35  40  51  67  79 104 111 129 138 156 166 189 204 210 212\n",
      " 214 215 216 217 223 230 234 239 255 259 293 295 301 306 310 311 317 346\n",
      " 348 349 365 374 391 417 431 432]\n",
      "Coefficients : \n",
      " [ -17.90900199 -239.72634104  547.0748194   323.75996659 -685.01029216\n",
      "  420.45633495   16.24736477   97.00510223  690.56606469   53.24313359]\n",
      "Mean squared error is: 3359.20\n",
      "Variance score is: 0.47\n",
      "\n",
      "TRAIN: [  0   1   2   4   5   6   7   8   9  10  11  12  14  15  17  18  20  21\n",
      "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
      "  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
      "  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75\n",
      "  76  78  79  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
      "  97  98  99 100 101 102 103 104 105 106 107 108 111 112 113 114 115 116\n",
      " 117 118 119 120 121 122 123 124 126 127 128 129 130 131 132 133 134 135\n",
      " 136 137 138 140 141 142 143 144 145 146 147 148 150 151 153 154 155 156\n",
      " 157 158 159 160 162 163 164 165 166 167 168 169 170 171 172 173 174 175\n",
      " 176 177 178 179 180 181 183 185 186 187 188 189 190 191 192 193 194 195\n",
      " 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213\n",
      " 214 215 216 217 218 219 220 221 222 223 224 225 227 229 230 231 232 233\n",
      " 234 235 236 238 239 240 241 242 243 244 245 246 247 249 250 251 252 253\n",
      " 254 255 256 257 259 261 262 263 264 265 266 267 268 270 271 272 273 274\n",
      " 276 277 278 279 281 282 283 284 285 287 288 289 290 291 292 293 295 296\n",
      " 297 298 300 301 302 303 304 305 306 308 310 311 313 314 315 316 317 319\n",
      " 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
      " 338 339 341 343 344 345 346 347 348 349 351 352 353 356 357 358 359 360\n",
      " 361 362 363 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379\n",
      " 380 381 382 383 385 386 387 388 389 390 391 392 393 394 396 397 398 399\n",
      " 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 417 418\n",
      " 419 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 437 438\n",
      " 440 441] TEST: [  3  13  16  19  77  80  83 109 110 125 139 149 152 161 182 184 226 228\n",
      " 237 248 258 260 269 275 280 286 294 299 307 309 312 318 340 342 350 354\n",
      " 355 364 384 395 416 420 436 439]\n",
      "Coefficients : \n",
      " [  31.1848371  -217.473176    517.05074312  314.16057538 -847.66515821\n",
      "  502.31471472  154.95508562  204.36324508  777.03080773   55.91647401]\n",
      "Mean squared error is: 2644.81\n",
      "Variance score is: 0.52\n",
      "\n",
      "TRAIN: [  1   2   3   4   5   6   7   8   9  10  12  13  14  15  16  17  18  19\n",
      "  20  21  22  24  25  26  27  28  29  30  31  32  33  34  35  37  38  39\n",
      "  40  42  44  45  46  47  49  51  52  53  54  55  56  57  59  60  61  63\n",
      "  64  65  66  67  68  70  71  72  73  74  75  76  77  78  79  80  81  82\n",
      "  83  84  85  87  88  89  90  91  92  93  96  97  99 100 101 102 103 104\n",
      " 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n",
      " 124 125 126 127 128 129 131 132 133 134 135 136 137 138 139 140 141 142\n",
      " 144 145 146 147 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 170 171 172 173 174 175 176 177 179 180 181 182\n",
      " 183 184 185 186 188 189 190 191 192 193 194 195 196 197 198 199 200 202\n",
      " 203 204 205 206 208 210 211 212 213 214 215 216 217 218 219 220 221 223\n",
      " 224 225 226 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242\n",
      " 243 244 245 246 247 248 249 250 251 252 253 254 255 257 258 259 260 261\n",
      " 262 263 264 265 267 268 269 271 272 273 274 275 276 277 278 280 281 282\n",
      " 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300\n",
      " 301 302 303 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320\n",
      " 321 322 323 324 325 326 327 328 329 330 332 333 334 335 336 337 338 339\n",
      " 340 342 343 344 345 346 347 348 349 350 353 354 355 356 357 358 359 360\n",
      " 361 362 363 364 365 366 367 369 370 371 372 373 374 375 376 378 380 381\n",
      " 382 383 384 385 386 387 388 389 391 392 393 394 395 396 397 398 399 400\n",
      " 401 402 403 404 405 406 407 408 410 411 412 413 414 415 416 417 418 419\n",
      " 420 421 422 424 425 426 427 428 430 431 432 433 434 435 436 437 438 439\n",
      " 440 441] TEST: [  0  11  23  36  41  43  48  50  58  62  69  86  94  95  98 123 130 143\n",
      " 148 169 178 187 201 207 209 222 227 256 266 270 279 304 305 331 341 351\n",
      " 352 368 377 379 390 409 423 429]\n",
      "Coefficients : \n",
      " [ -26.88391603 -252.0397439   503.41356402  315.2773326  -571.40958548\n",
      "  286.51129946   28.05463113  225.11809482  666.75509602   89.42910346]\n",
      "Mean squared error is: 3098.28\n",
      "Variance score is: 0.51\n",
      "\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  29  30  33  34  35  36  37  39\n",
      "  40  41  43  44  45  46  47  48  49  50  51  52  54  55  56  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  83  85  86  87  88  89  90  92  93  94  95  96  97  98  99\n",
      " 100 101 102 103 104 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
      " 120 122 123 124 125 126 129 130 132 133 134 135 136 137 138 139 140 141\n",
      " 142 143 144 145 146 147 148 149 150 152 153 154 155 156 157 158 159 160\n",
      " 161 162 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 181 182 184 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200\n",
      " 201 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220\n",
      " 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238\n",
      " 239 240 241 242 243 245 246 247 248 249 250 251 252 253 254 255 256 258\n",
      " 259 260 261 263 264 265 266 268 269 270 271 272 274 275 276 277 278 279\n",
      " 280 281 282 283 284 285 286 287 288 289 292 293 294 295 296 297 298 299\n",
      " 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
      " 318 319 320 322 323 325 326 327 328 329 330 331 332 333 335 336 337 338\n",
      " 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 357\n",
      " 359 360 361 362 363 364 365 366 367 368 369 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 384 386 387 388 389 390 391 393 395 396 397 398 399\n",
      " 400 401 402 403 404 405 407 409 410 411 412 413 414 415 416 417 418 419\n",
      " 420 421 422 423 424 426 427 428 429 430 431 432 433 434 435 436 437 438\n",
      " 439 441] TEST: [ 28  31  32  38  42  53  57  82  84  91 105 119 121 127 128 131 151 163\n",
      " 180 183 185 202 203 244 257 262 267 273 290 291 321 324 334 356 358 370\n",
      " 383 385 392 394 406 408 425 440]\n",
      "Coefficients : \n",
      " [ -24.79835572 -240.65495279  497.13314739  330.01195479 -899.61628926\n",
      "  563.92615033  143.71696024  188.11876918  787.95790282   85.32721237]\n",
      "Mean squared error is: 2254.54\n",
      "Variance score is: 0.66\n",
      "\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  26  27  28  29  30  31  32  33  34  35  36  37\n",
      "  38  40  41  42  43  44  45  46  48  49  50  51  52  53  54  55  56  57\n",
      "  58  59  60  61  62  63  64  65  66  67  68  69  71  73  74  75  76  77\n",
      "  78  79  80  81  82  83  84  85  86  89  90  91  92  93  94  95  96  97\n",
      "  98 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 116 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
      " 137 138 139 140 141 142 143 144 145 146 148 149 150 151 152 153 154 155\n",
      " 156 157 158 159 160 161 162 163 164 166 167 168 169 170 171 173 175 176\n",
      " 178 179 180 181 182 183 184 185 186 187 188 189 190 191 194 196 198 199\n",
      " 200 201 202 203 204 205 206 207 208 209 210 212 213 214 215 216 217 218\n",
      " 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236\n",
      " 237 238 239 240 241 244 245 246 247 248 249 250 252 253 254 255 256 257\n",
      " 258 259 260 261 262 263 264 266 267 268 269 270 271 272 273 274 275 276\n",
      " 278 279 280 281 282 283 284 286 287 289 290 291 293 294 295 296 297 298\n",
      " 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 315 316 317\n",
      " 318 319 320 321 322 324 325 326 327 328 329 330 331 332 334 336 339 340\n",
      " 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 397 399 400 401 403 405 406 407 408 409 411 412 414 415 416 417 418 419\n",
      " 420 421 422 423 424 425 427 429 430 431 432 433 434 435 436 437 438 439\n",
      " 440 441] TEST: [  9  25  39  47  70  72  87  88  99 115 117 147 165 172 174 177 192 193\n",
      " 195 197 211 242 243 251 265 277 285 288 292 314 323 333 335 337 338 359\n",
      " 396 398 402 404 410 413 426 428]\n",
      "Coefficients : \n",
      " [  -5.57118766 -235.71226304  521.6995965   336.11656199 -974.96874459\n",
      "  614.42336915  189.51381317  210.61738584  755.04229697   88.32210917]\n",
      "Mean squared error is: 3117.79\n",
      "Variance score is: 0.55\n",
      "\n",
      "The average variance score is: 0.48386683906671085\n",
      "The standard deviation of variance scores is: 0.11301537460659498\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "seed=np.random.RandomState(0)\n",
    "variance_score_list=[]\n",
    "regr = linear_model.LinearRegression()\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_X = diabetes.data\n",
    "diabetes_y= diabetes.target\n",
    "kf = KFold(n_splits=10, random_state=seed,shuffle=True)\n",
    "print(kf.get_n_splits(diabetes_X))\n",
    "print(kf)\n",
    "for train_index, test_index in kf.split(diabetes_X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = diabetes_X[train_index], diabetes_X[test_index]\n",
    "    y_train, y_test = diabetes_y[train_index], diabetes_y[test_index]\n",
    "    regr.fit(X_train, y_train)\n",
    "    diabetes_y_pred = regr.predict(X_test)\n",
    "     # The coefficients\n",
    "    print('Coefficients : \\n', regr.coef_)\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error is: %.2f\"\n",
    "    % mean_squared_error(y_test, diabetes_y_pred))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score is: %.2f' % r2_score(y_test, diabetes_y_pred))\n",
    "    variance_score_list.append(r2_score(y_test, diabetes_y_pred))\n",
    "    print()\n",
    "print(\"The average variance score is: \"+ str(np.mean(variance_score_list)))\n",
    "print(\"The standard deviation of variance scores is: \"+ str(np.std(variance_score_list)))\n",
    "#print(diabetes.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "AIC for the 1 model is: 572.7196292626445\n",
      "BIC for the 1 model is: 574.1208266443067\n",
      "AIC for the 2 model is: 578.7196292626445\n",
      "BIC for the 2 model is: 584.3244187892931\n",
      "AIC for the 3 model is: 600.7196292626445\n",
      "BIC for the 3 model is: 621.7375899875768\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np #import numpy\n",
    "import matplotlib.pyplot as plt #import matplotlib\n",
    "from sklearn.pipeline import Pipeline #import pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures #import preprocessing\n",
    "from sklearn.linear_model import LinearRegression #import linear regression\n",
    "from sklearn.model_selection import cross_val_score #import model selection\n",
    "\n",
    "\n",
    "def true_fun(X): #function declaration of true_fun which takes in a parameter X\n",
    "    return np.cos(1.5 * np.pi * X) #return the result of cosine(1.5*pi*x)\n",
    "\n",
    "np.random.seed(0)#generate a random seed so that the random generation could be regenerated\n",
    "\n",
    "n_samples = 30 #declare a new variable called n_sample and set it to 30\n",
    "degrees = [1, 4, 15] # declare a new list degrees\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples)) #randomly generate 30 numbers and then sort them and store the sorted results in X\n",
    "\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1# randomly generate n_samples standard normal dataset and add those number to the results of cosine(1.5*pi*X) and store the results in y\n",
    "\n",
    "plt.figure(figsize=(14, 5)) #set plt figuresize tp 14*5 (inches)\n",
    "for i in range(len(degrees)): #for loop initiation\n",
    "    ax = plt.subplot(1, len(degrees), i + 1) #declare 1row * len(degrees)col subplots for each degree \n",
    "    plt.setp(ax, xticks=(), yticks=()) #set plot style for each ax\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i], #generate a feature matrix consisting of all polynomial combination of the features with degree less than or equal to degree degrees[i]\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()#declare a new linear regression model\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)]) #pipeline polynomial_feature generation and linear_regressoion model generation\n",
    "    pipeline.fit(X[:, np.newaxis], y) #fit the pipeline to the transposed X and y\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)#score the cross-validation and calculate the R^2 score of the model\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100) #return an array consisting of the evenly spaced numbers between 0 and 1 with an interval of 0.01\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")#plot the model prediction results\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")#plot the true function results\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")#plot the sample \n",
    "    plt.xlabel(\"x\")#set the label of x-axis of the plots to x\n",
    "    plt.ylabel(\"y\")#set the label of y-axis of the plots to y\n",
    "    plt.xlim((0, 1))#set the x limits of the current axes to from 0 to 1\n",
    "    plt.ylim((-2, 2))#set the y limits of the current axes to from -2 to 2\n",
    "    plt.legend(loc=\"best\")#place a legend on the axes where it is the best location\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degrees[i], -scores.mean(), scores.std()))#set the plot title to degree, MSE score +/- the standard deviation of the MSE scores\n",
    "for i in range(0, len(degrees)):\n",
    "    print(\"AIC for the \"+str(i+1)+\" model is: \"+str((2*degrees[i]) + n_samples * np.log(-scores.mean())))\n",
    "    print(\"BIC for the \"+str(i+1)+\" model is: \"+str(degrees[i]* np.log(n_samples) + n_samples * np.log(-scores.mean())))\n",
    "plt.show()#show the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.200e+01 8.000e+00 3.070e+02 ... 7.000e+01 1.000e+00 3.504e+03]\n",
      " [1.150e+01 8.000e+00 3.500e+02 ... 7.000e+01 1.000e+00 3.693e+03]\n",
      " [1.100e+01 8.000e+00 3.180e+02 ... 7.000e+01 1.000e+00 3.436e+03]\n",
      " ...\n",
      " [1.160e+01 4.000e+00 1.350e+02 ... 8.200e+01 1.000e+00 2.295e+03]\n",
      " [1.860e+01 4.000e+00 1.200e+02 ... 8.200e+01 1.000e+00 2.625e+03]\n",
      " [1.940e+01 4.000e+00 1.190e+02 ... 8.200e+01 1.000e+00 2.720e+03]]\n",
      "[[18. ]\n",
      " [15. ]\n",
      " [18. ]\n",
      " [16. ]\n",
      " [17. ]\n",
      " [15. ]\n",
      " [14. ]\n",
      " [14. ]\n",
      " [14. ]\n",
      " [15. ]\n",
      " [15. ]\n",
      " [14. ]\n",
      " [15. ]\n",
      " [14. ]\n",
      " [24. ]\n",
      " [22. ]\n",
      " [18. ]\n",
      " [21. ]\n",
      " [27. ]\n",
      " [26. ]\n",
      " [25. ]\n",
      " [24. ]\n",
      " [25. ]\n",
      " [26. ]\n",
      " [21. ]\n",
      " [10. ]\n",
      " [10. ]\n",
      " [11. ]\n",
      " [ 9. ]\n",
      " [27. ]\n",
      " [28. ]\n",
      " [25. ]\n",
      " [19. ]\n",
      " [16. ]\n",
      " [17. ]\n",
      " [19. ]\n",
      " [18. ]\n",
      " [14. ]\n",
      " [14. ]\n",
      " [14. ]\n",
      " [14. ]\n",
      " [12. ]\n",
      " [13. ]\n",
      " [13. ]\n",
      " [18. ]\n",
      " [22. ]\n",
      " [19. ]\n",
      " [18. ]\n",
      " [23. ]\n",
      " [28. ]\n",
      " [30. ]\n",
      " [30. ]\n",
      " [31. ]\n",
      " [35. ]\n",
      " [27. ]\n",
      " [26. ]\n",
      " [24. ]\n",
      " [25. ]\n",
      " [23. ]\n",
      " [20. ]\n",
      " [21. ]\n",
      " [13. ]\n",
      " [14. ]\n",
      " [15. ]\n",
      " [14. ]\n",
      " [17. ]\n",
      " [11. ]\n",
      " [13. ]\n",
      " [12. ]\n",
      " [13. ]\n",
      " [19. ]\n",
      " [15. ]\n",
      " [13. ]\n",
      " [13. ]\n",
      " [14. ]\n",
      " [18. ]\n",
      " [22. ]\n",
      " [21. ]\n",
      " [26. ]\n",
      " [22. ]\n",
      " [28. ]\n",
      " [23. ]\n",
      " [28. ]\n",
      " [27. ]\n",
      " [13. ]\n",
      " [14. ]\n",
      " [13. ]\n",
      " [14. ]\n",
      " [15. ]\n",
      " [12. ]\n",
      " [13. ]\n",
      " [13. ]\n",
      " [14. ]\n",
      " [13. ]\n",
      " [12. ]\n",
      " [13. ]\n",
      " [18. ]\n",
      " [16. ]\n",
      " [18. ]\n",
      " [18. ]\n",
      " [23. ]\n",
      " [26. ]\n",
      " [11. ]\n",
      " [12. ]\n",
      " [13. ]\n",
      " [12. ]\n",
      " [18. ]\n",
      " [20. ]\n",
      " [21. ]\n",
      " [22. ]\n",
      " [18. ]\n",
      " [19. ]\n",
      " [21. ]\n",
      " [26. ]\n",
      " [15. ]\n",
      " [16. ]\n",
      " [29. ]\n",
      " [24. ]\n",
      " [20. ]\n",
      " [19. ]\n",
      " [15. ]\n",
      " [24. ]\n",
      " [20. ]\n",
      " [11. ]\n",
      " [20. ]\n",
      " [19. ]\n",
      " [15. ]\n",
      " [31. ]\n",
      " [26. ]\n",
      " [32. ]\n",
      " [25. ]\n",
      " [16. ]\n",
      " [16. ]\n",
      " [18. ]\n",
      " [16. ]\n",
      " [13. ]\n",
      " [14. ]\n",
      " [14. ]\n",
      " [14. ]\n",
      " [29. ]\n",
      " [26. ]\n",
      " [26. ]\n",
      " [31. ]\n",
      " [32. ]\n",
      " [28. ]\n",
      " [24. ]\n",
      " [26. ]\n",
      " [24. ]\n",
      " [26. ]\n",
      " [31. ]\n",
      " [19. ]\n",
      " [18. ]\n",
      " [15. ]\n",
      " [15. ]\n",
      " [16. ]\n",
      " [15. ]\n",
      " [16. ]\n",
      " [14. ]\n",
      " [17. ]\n",
      " [16. ]\n",
      " [15. ]\n",
      " [18. ]\n",
      " [21. ]\n",
      " [20. ]\n",
      " [13. ]\n",
      " [29. ]\n",
      " [23. ]\n",
      " [20. ]\n",
      " [23. ]\n",
      " [24. ]\n",
      " [25. ]\n",
      " [24. ]\n",
      " [18. ]\n",
      " [29. ]\n",
      " [19. ]\n",
      " [23. ]\n",
      " [23. ]\n",
      " [22. ]\n",
      " [25. ]\n",
      " [33. ]\n",
      " [28. ]\n",
      " [25. ]\n",
      " [25. ]\n",
      " [26. ]\n",
      " [27. ]\n",
      " [17.5]\n",
      " [16. ]\n",
      " [15.5]\n",
      " [14.5]\n",
      " [22. ]\n",
      " [22. ]\n",
      " [24. ]\n",
      " [22.5]\n",
      " [29. ]\n",
      " [24.5]\n",
      " [29. ]\n",
      " [33. ]\n",
      " [20. ]\n",
      " [18. ]\n",
      " [18.5]\n",
      " [17.5]\n",
      " [29.5]\n",
      " [32. ]\n",
      " [28. ]\n",
      " [26.5]\n",
      " [20. ]\n",
      " [13. ]\n",
      " [19. ]\n",
      " [19. ]\n",
      " [16.5]\n",
      " [16.5]\n",
      " [13. ]\n",
      " [13. ]\n",
      " [13. ]\n",
      " [31.5]\n",
      " [30. ]\n",
      " [36. ]\n",
      " [25.5]\n",
      " [33.5]\n",
      " [17.5]\n",
      " [17. ]\n",
      " [15.5]\n",
      " [15. ]\n",
      " [17.5]\n",
      " [20.5]\n",
      " [19. ]\n",
      " [18.5]\n",
      " [16. ]\n",
      " [15.5]\n",
      " [15.5]\n",
      " [16. ]\n",
      " [29. ]\n",
      " [24.5]\n",
      " [26. ]\n",
      " [25.5]\n",
      " [30.5]\n",
      " [33.5]\n",
      " [30. ]\n",
      " [30.5]\n",
      " [22. ]\n",
      " [21.5]\n",
      " [21.5]\n",
      " [43.1]\n",
      " [36.1]\n",
      " [32.8]\n",
      " [39.4]\n",
      " [36.1]\n",
      " [19.9]\n",
      " [19.4]\n",
      " [20.2]\n",
      " [19.2]\n",
      " [20.5]\n",
      " [20.2]\n",
      " [25.1]\n",
      " [20.5]\n",
      " [19.4]\n",
      " [20.6]\n",
      " [20.8]\n",
      " [18.6]\n",
      " [18.1]\n",
      " [19.2]\n",
      " [17.7]\n",
      " [18.1]\n",
      " [17.5]\n",
      " [30. ]\n",
      " [27.5]\n",
      " [27.2]\n",
      " [30.9]\n",
      " [21.1]\n",
      " [23.2]\n",
      " [23.8]\n",
      " [23.9]\n",
      " [20.3]\n",
      " [17. ]\n",
      " [21.6]\n",
      " [16.2]\n",
      " [31.5]\n",
      " [29.5]\n",
      " [21.5]\n",
      " [19.8]\n",
      " [22.3]\n",
      " [20.2]\n",
      " [20.6]\n",
      " [17. ]\n",
      " [17.6]\n",
      " [16.5]\n",
      " [18.2]\n",
      " [16.9]\n",
      " [15.5]\n",
      " [19.2]\n",
      " [18.5]\n",
      " [31.9]\n",
      " [34.1]\n",
      " [35.7]\n",
      " [27.4]\n",
      " [25.4]\n",
      " [23. ]\n",
      " [27.2]\n",
      " [23.9]\n",
      " [34.2]\n",
      " [34.5]\n",
      " [31.8]\n",
      " [37.3]\n",
      " [28.4]\n",
      " [28.8]\n",
      " [26.8]\n",
      " [33.5]\n",
      " [41.5]\n",
      " [38.1]\n",
      " [32.1]\n",
      " [37.2]\n",
      " [28. ]\n",
      " [26.4]\n",
      " [24.3]\n",
      " [19.1]\n",
      " [34.3]\n",
      " [29.8]\n",
      " [31.3]\n",
      " [37. ]\n",
      " [32.2]\n",
      " [46.6]\n",
      " [27.9]\n",
      " [40.8]\n",
      " [44.3]\n",
      " [43.4]\n",
      " [36.4]\n",
      " [30. ]\n",
      " [44.6]\n",
      " [33.8]\n",
      " [29.8]\n",
      " [32.7]\n",
      " [23.7]\n",
      " [35. ]\n",
      " [32.4]\n",
      " [27.2]\n",
      " [26.6]\n",
      " [25.8]\n",
      " [23.5]\n",
      " [30. ]\n",
      " [39.1]\n",
      " [39. ]\n",
      " [35.1]\n",
      " [32.3]\n",
      " [37. ]\n",
      " [37.7]\n",
      " [34.1]\n",
      " [34.7]\n",
      " [34.4]\n",
      " [29.9]\n",
      " [33. ]\n",
      " [33.7]\n",
      " [32.4]\n",
      " [32.9]\n",
      " [31.6]\n",
      " [28.1]\n",
      " [30.7]\n",
      " [25.4]\n",
      " [24.2]\n",
      " [22.4]\n",
      " [26.6]\n",
      " [20.2]\n",
      " [17.6]\n",
      " [28. ]\n",
      " [27. ]\n",
      " [34. ]\n",
      " [31. ]\n",
      " [29. ]\n",
      " [27. ]\n",
      " [24. ]\n",
      " [36. ]\n",
      " [37. ]\n",
      " [31. ]\n",
      " [38. ]\n",
      " [36. ]\n",
      " [36. ]\n",
      " [36. ]\n",
      " [34. ]\n",
      " [38. ]\n",
      " [32. ]\n",
      " [38. ]\n",
      " [25. ]\n",
      " [38. ]\n",
      " [26. ]\n",
      " [22. ]\n",
      " [32. ]\n",
      " [36. ]\n",
      " [27. ]\n",
      " [27. ]\n",
      " [44. ]\n",
      " [32. ]\n",
      " [28. ]\n",
      " [31. ]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'linear_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a9297c446e8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#print(target_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mregr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'linear_model' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "#read in dataset without carname column\n",
    "cols=[0,1,2,3,4,5,6,7]\n",
    "\n",
    "test_size=20\n",
    "df=pd.read_csv(\"auto-mpg.csv\", usecols=cols, index_col=False)\n",
    "#clean data\n",
    "df.replace('?', np.NaN,inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.horsepower = df.horsepower.astype('float')\n",
    "#print(df)\n",
    "#scale data to zero mean and unit variance\n",
    "#print(scaled_df)\n",
    "#print(scaled_df.mean(axis=0))\n",
    "#print(scaled_df.std(axis=0))\n",
    "y_col=[\"mpg\"]\n",
    "data_col=[\"cylinders\",\"displacement\",\"horsepower\",\"weight\",\"acceleration\",\"model year\",\"origin\"]\n",
    "\n",
    "\n",
    "selected_target =df[y_col]\n",
    "selected_data = df[data_col]\n",
    "#print(selected_target)\n",
    "#print(selected_data)\n",
    "# Convert data and target into dicts, T is used to transpose\n",
    "dict_data = selected_data.T.to_dict('dict').values()\n",
    "dict_target = selected_target.T.to_dict('dict').values()\n",
    "#print(dict_data)\n",
    "#print(dict_target)\n",
    "# Make new vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform data and target\n",
    "data = vectorizer.fit_transform(dict_data)\n",
    "target = vectorizer.fit_transform(dict_target)\n",
    "print(data)\n",
    "print(target)\n",
    "# We really need standardizing the data in this case.\n",
    "preprocessing.scale(data, copy=False)\n",
    "\n",
    "# Split data and target into train and test data\n",
    "data_train = data[:-test_size]\n",
    "data_test = data[-test_size:]\n",
    "#print(data_train)\n",
    "target_train = target[:-test_size]\n",
    "target_test = target[-test_size:]\n",
    "#print(target_train)\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(data_train, target_train)\n",
    "\n",
    "y_predicted = regr.predict(data_test)\n",
    "y_true = target_test\n",
    "\n",
    "print(\"Regression Coefficients: \", regr.coef_)\n",
    "print(\"Variance explained, calculated by SK-learn method: %.2f\" % regr.score(data_test, target_test))\n",
    "print(\"Mean Squared Error, version 2: \" + str(mean_squared_error(y_true, y_predicted, multioutput='raw_values')))\n",
    "print(\"residual sum of squares: \"+ str(((y_predicted - y_true) ** 2).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC for all coefficients: 1171.65\n",
      "BIC for all coefficients: 1203.42\n",
      "Regression block 1 Coefficients:  [[-94.72429178]]\n",
      "Regression block 1 Intercept:  [5179.98893954]\n",
      "AIC for the the largest magnitude coefficients: 5704.76\n",
      "BIC for the largest magnitude coefficients: 7265.47\n"
     ]
    }
   ],
   "source": [
    "# AIC and BIC for All coefficients\n",
    "k = len(data_col)+1\n",
    "n= df.shape[0]\n",
    "mean_sq_er_total = mean_squared_error(y_true, y_predicted, multioutput='raw_values')\n",
    "aic_total = (2*k) + n * np.log(mean_sq_er_total)\n",
    "print(\"AIC for all coefficients: %.2f\" % aic_total)\n",
    "bic_total = k * np.log(n) + n * np.log(mean_sq_er_total)\n",
    "print(\"BIC for all coefficients: %.2f\" % bic_total)\n",
    "\n",
    "\n",
    "#the model with the largest magnitude coefficient\n",
    "x = df[\"mpg\"].values.reshape(-1,1)\n",
    "y = df[\"weight\"].values.reshape(-1,1)\n",
    "\n",
    "x_train = x[:-test_size]\n",
    "x_test = x[-test_size:]\n",
    "#print(data_train)\n",
    "y_train = y[:-test_size]\n",
    "y_test = y[-test_size:]\n",
    "\n",
    "simple_regr = linear_model.LinearRegression()\n",
    "simple_regr.fit(x_train, y_train)\n",
    "\n",
    "k = len(y)+1\n",
    "n= df.shape[0]\n",
    "\n",
    "print(\"Regression block 1 Coefficients: \", simple_regr.coef_)\n",
    "print(\"Regression block 1 Intercept: \", simple_regr.intercept_)\n",
    "\n",
    "y_predicted = simple_regr.predict(x_test)\n",
    "y_true = y_test\n",
    "\n",
    "mean_sq_er_total = mean_squared_error(y_true, y_predicted, multioutput='raw_values')\n",
    "# AIC and BIC for the largest magnitude coefficients\n",
    "aic_total = (2*k) + n * np.log(mean_sq_er_total)\n",
    "print(\"AIC for the the largest magnitude coefficients: %.2f\" % aic_total)\n",
    "bic_total = k * np.log(n) + n * np.log(mean_sq_er_total)\n",
    "print(\"BIC for the largest magnitude coefficients: %.2f\" % bic_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC for selected coefficients: 1169.65\n",
      "BIC for selected coefficients: 1197.45\n"
     ]
    }
   ],
   "source": [
    "y_col=[\"mpg\"]\n",
    "data_col=[\"displacement\",\"horsepower\",\"weight\",\"acceleration\",\"model year\",\"origin\"]\n",
    "#\"cylinders\",\"displacement\",\"horsepower\",\"weight\",\"acceleration\",\"model year\",\"origin\"\n",
    "\"\"\"\n",
    "3-factor ones:\n",
    "\"cylinders\",\"displacement\",\"horsepower\", \"cylinders\",\"displacement\",\"weight\", \"cylinders\",\"displacement\",\"acceleration\"\n",
    "\"cylinders\",\"displacement\",\"model year\", \"cylinders\",\"displacement\",\"origin\", \"cylinders\",\"horsepower\",\"weight\", \n",
    "\"cylinders\",\"horsepower\",\"acceleration\", \"cylinders\",\"horsepower\",\"model year\", \"cylinders\",\"horsepower\",\"origin\"\n",
    "\"cylinders\",\"weight\",\"acceleration\", \"cylinders\",\"weight\",\"model year\", \"cylinders\",\"weight\",\"origin\"\n",
    "\"cylinders\",\"acceleration\",\"model year\", \"cylinders\",\"acceleration\",\"origin\", \"cylinders\",\"model year\",\"origin\"\n",
    "\"displacement\",\"horsepower\",\"weight\", \"displacement\",\"horsepower\",\"acceleration\", \"displacement\",\"horsepower\",\"model year\"\n",
    "\"displacement\",\"horsepower\",\"origin\", \"displacement\",\"weight\",\"acceleration\", \"displacement\",\"weight\",\"model year\"\n",
    "\"displacement\",\"weight\",\"origin\", \"displacement\",\"acceleration\",\"model year\", \"displacement\",\"acceleration\",\"origin\"\n",
    "\"displacement\",\"model year\",\"origin\", \"horsepower\",\"weight\",\"acceleration\", \"horsepower\",\"weight\",\"model year\"\n",
    "\"horsepower\",\"weight\",\"origin\", \"weight\",\"acceleration\",\"model year\", \"weight\",\"acceleration\",\"origin\"\n",
    "\"acceleration\",\"model year\",\"origin\"\n",
    "\n",
    "4-factor ones:\n",
    "\"cylinders\",\"displacement\",\"horsepower\",\"weight\", \"cylinders\",\"displacement\",\"horsepower\",\"acceleration\"\n",
    "\"cylinders\",\"displacement\",\"horsepower\",\"model year\", \"cylinders\",\"displacement\",\"horsepower\",\"origin\"\n",
    "\"cylinders\",\"displacement\",\"weight\",\"acceleration\", \"cylinders\",\"displacement\",\"weight\",\"model year\"\n",
    "\"cylinders\",\"displacement\",\"weight\",\"origin\", \"cylinders\",\"displacement\",\"acceleration\",\"model year\"\n",
    "\"cylinders\",\"displacement\",\"acceleration\",\"origin\", \"cylinders\",\"displacement\",\"model year\",\"origin\"\n",
    "...\n",
    "\"\"\"\n",
    "selected_target2 =df[y_col]\n",
    "selected_data2 = df[data_col]\n",
    "#print(selected_target)\n",
    "#print(selected_data)\n",
    "# Convert data and target into dicts, T is used to transpose\n",
    "dict_data2 = selected_data2.T.to_dict('dict').values()\n",
    "dict_target2 = selected_target2.T.to_dict('dict').values()\n",
    "#print(dict_data)\n",
    "#print(dict_target)\n",
    "# Make new vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform data and target\n",
    "data_2 = vectorizer.fit_transform(dict_data)\n",
    "target_2 = vectorizer.fit_transform(dict_target)\n",
    "#print(data)\n",
    "#print(target)\n",
    "# We really need standardizing the data in this case.\n",
    "preprocessing.scale(data_2, copy=False)\n",
    "\n",
    "# Split data and target into train and test data\n",
    "data2_train = data_2[:-test_size]\n",
    "data2_test = data_2[-test_size:]\n",
    "#print(data_train)\n",
    "target2_train = target_2[:-test_size]\n",
    "target2_test = target_2[-test_size:]\n",
    "#print(target_train)\n",
    "\n",
    "regr2 = linear_model.LinearRegression()\n",
    "regr2.fit(data2_train, target2_train)\n",
    "\n",
    "y2_predicted = regr2.predict(data2_test)\n",
    "y2_true = target2_test\n",
    "\n",
    "\n",
    "k = len(data_col)+1\n",
    "n= selected_data.shape[0]\n",
    "mean_sq_er_total = mean_squared_error(y2_true, y2_predicted, multioutput='raw_values')\n",
    "aic_total = (2*k) + n * np.log(mean_sq_er_total)\n",
    "print(\"AIC for selected coefficients: %.2f\" % aic_total)\n",
    "bic_total = k * np.log(n) + n * np.log(mean_sq_er_total)\n",
    "print(\"BIC for selected coefficients: %.2f\" % bic_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold scores are\n",
      "[0.6467103  0.77241016 0.59004729 0.7865709  0.80982887 0.83918792\n",
      " 0.57751578 0.75474699 0.10028113 0.54874121]\n",
      "Average Accuracy = 0.64\n",
      "10 fold scores are\n",
      "[ 0.70329971  0.66326216  0.59529927  0.69629871  0.67052312  0.80611248\n",
      "  0.70120512  0.7333608  -1.76501878 -0.22454833]\n",
      "Average Accuracy = 0.36\n",
      "10 fold scores are\n",
      "[0.6467103  0.77241016 0.59004729 0.7865709  0.80982887 0.83918792\n",
      " 0.57751578 0.75474699 0.10028113 0.54874121]\n",
      "Average Accuracy = 0.64\n"
     ]
    }
   ],
   "source": [
    "#MSE for the model with all coefficients\n",
    "scores = cross_val_score(regr, data, target, cv=10)\n",
    "print(\"10 fold scores are\")\n",
    "print(scores)\n",
    "print(\"Average Accuracy = %.2f\" % np.mean(scores))\n",
    "\n",
    "\n",
    "\n",
    "#MSE for the model with the largest coefficient\n",
    "scores = cross_val_score(simple_regr, x, y, cv=10)\n",
    "print(\"10 fold scores are\")\n",
    "print(scores)\n",
    "print(\"Average Accuracy = %.2f\" % np.mean(scores))\n",
    "\n",
    "#MSE for the best AIC/BIC\n",
    "scores = cross_val_score(regr2, data_2, target_2, cv=10)\n",
    "print(\"10 fold scores are\")\n",
    "print(scores)\n",
    "print(\"Average Accuracy = %.2f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   8.   307.   130.  ...  840.    12.    70. ]\n",
      " [   8.   350.   165.  ...  805.    11.5   70. ]\n",
      " [   8.   318.   150.  ...  770.    11.    70. ]\n",
      " ...\n",
      " [   4.   135.    84.  ...  951.2   11.6   82. ]\n",
      " [   4.   120.    79.  ... 1525.2   18.6   82. ]\n",
      " [   4.   119.    82.  ... 1590.8   19.4   82. ]]\n",
      "[[18. ]\n",
      " [15. ]\n",
      " [18. ]\n",
      " [16. ]\n",
      " [17. ]\n",
      " [15. ]\n",
      " [14. ]\n",
      " [14. ]\n",
      " [14. ]\n",
      " [15. ]\n",
      " [15. ]\n",
      " [14. ]\n",
      " [15. ]\n",
      " [14. ]\n",
      " [24. ]\n",
      " [22. ]\n",
      " [18. ]\n",
      " [21. ]\n",
      " [27. ]\n",
      " [26. ]\n",
      " [25. ]\n",
      " [24. ]\n",
      " [25. ]\n",
      " [26. ]\n",
      " [21. ]\n",
      " [10. ]\n",
      " [10. ]\n",
      " [11. ]\n",
      " [ 9. ]\n",
      " [27. ]\n",
      " [28. ]\n",
      " [25. ]\n",
      " [19. ]\n",
      " [16. ]\n",
      " [17. ]\n",
      " [19. ]\n",
      " [18. ]\n",
      " [14. ]\n",
      " [14. ]\n",
      " [14. ]\n",
      " [14. ]\n",
      " [12. ]\n",
      " [13. ]\n",
      " [13. ]\n",
      " [18. ]\n",
      " [22. ]\n",
      " [19. ]\n",
      " [18. ]\n",
      " [23. ]\n",
      " [28. ]\n",
      " [30. ]\n",
      " [30. ]\n",
      " [31. ]\n",
      " [35. ]\n",
      " [27. ]\n",
      " [26. ]\n",
      " [24. ]\n",
      " [25. ]\n",
      " [23. ]\n",
      " [20. ]\n",
      " [21. ]\n",
      " [13. ]\n",
      " [14. ]\n",
      " [15. ]\n",
      " [14. ]\n",
      " [17. ]\n",
      " [11. ]\n",
      " [13. ]\n",
      " [12. ]\n",
      " [13. ]\n",
      " [19. ]\n",
      " [15. ]\n",
      " [13. ]\n",
      " [13. ]\n",
      " [14. ]\n",
      " [18. ]\n",
      " [22. ]\n",
      " [21. ]\n",
      " [26. ]\n",
      " [22. ]\n",
      " [28. ]\n",
      " [23. ]\n",
      " [28. ]\n",
      " [27. ]\n",
      " [13. ]\n",
      " [14. ]\n",
      " [13. ]\n",
      " [14. ]\n",
      " [15. ]\n",
      " [12. ]\n",
      " [13. ]\n",
      " [13. ]\n",
      " [14. ]\n",
      " [13. ]\n",
      " [12. ]\n",
      " [13. ]\n",
      " [18. ]\n",
      " [16. ]\n",
      " [18. ]\n",
      " [18. ]\n",
      " [23. ]\n",
      " [26. ]\n",
      " [11. ]\n",
      " [12. ]\n",
      " [13. ]\n",
      " [12. ]\n",
      " [18. ]\n",
      " [20. ]\n",
      " [21. ]\n",
      " [22. ]\n",
      " [18. ]\n",
      " [19. ]\n",
      " [21. ]\n",
      " [26. ]\n",
      " [15. ]\n",
      " [16. ]\n",
      " [29. ]\n",
      " [24. ]\n",
      " [20. ]\n",
      " [19. ]\n",
      " [15. ]\n",
      " [24. ]\n",
      " [20. ]\n",
      " [11. ]\n",
      " [20. ]\n",
      " [19. ]\n",
      " [15. ]\n",
      " [31. ]\n",
      " [26. ]\n",
      " [32. ]\n",
      " [25. ]\n",
      " [16. ]\n",
      " [16. ]\n",
      " [18. ]\n",
      " [16. ]\n",
      " [13. ]\n",
      " [14. ]\n",
      " [14. ]\n",
      " [14. ]\n",
      " [29. ]\n",
      " [26. ]\n",
      " [26. ]\n",
      " [31. ]\n",
      " [32. ]\n",
      " [28. ]\n",
      " [24. ]\n",
      " [26. ]\n",
      " [24. ]\n",
      " [26. ]\n",
      " [31. ]\n",
      " [19. ]\n",
      " [18. ]\n",
      " [15. ]\n",
      " [15. ]\n",
      " [16. ]\n",
      " [15. ]\n",
      " [16. ]\n",
      " [14. ]\n",
      " [17. ]\n",
      " [16. ]\n",
      " [15. ]\n",
      " [18. ]\n",
      " [21. ]\n",
      " [20. ]\n",
      " [13. ]\n",
      " [29. ]\n",
      " [23. ]\n",
      " [20. ]\n",
      " [23. ]\n",
      " [24. ]\n",
      " [25. ]\n",
      " [24. ]\n",
      " [18. ]\n",
      " [29. ]\n",
      " [19. ]\n",
      " [23. ]\n",
      " [23. ]\n",
      " [22. ]\n",
      " [25. ]\n",
      " [33. ]\n",
      " [28. ]\n",
      " [25. ]\n",
      " [25. ]\n",
      " [26. ]\n",
      " [27. ]\n",
      " [17.5]\n",
      " [16. ]\n",
      " [15.5]\n",
      " [14.5]\n",
      " [22. ]\n",
      " [22. ]\n",
      " [24. ]\n",
      " [22.5]\n",
      " [29. ]\n",
      " [24.5]\n",
      " [29. ]\n",
      " [33. ]\n",
      " [20. ]\n",
      " [18. ]\n",
      " [18.5]\n",
      " [17.5]\n",
      " [29.5]\n",
      " [32. ]\n",
      " [28. ]\n",
      " [26.5]\n",
      " [20. ]\n",
      " [13. ]\n",
      " [19. ]\n",
      " [19. ]\n",
      " [16.5]\n",
      " [16.5]\n",
      " [13. ]\n",
      " [13. ]\n",
      " [13. ]\n",
      " [31.5]\n",
      " [30. ]\n",
      " [36. ]\n",
      " [25.5]\n",
      " [33.5]\n",
      " [17.5]\n",
      " [17. ]\n",
      " [15.5]\n",
      " [15. ]\n",
      " [17.5]\n",
      " [20.5]\n",
      " [19. ]\n",
      " [18.5]\n",
      " [16. ]\n",
      " [15.5]\n",
      " [15.5]\n",
      " [16. ]\n",
      " [29. ]\n",
      " [24.5]\n",
      " [26. ]\n",
      " [25.5]\n",
      " [30.5]\n",
      " [33.5]\n",
      " [30. ]\n",
      " [30.5]\n",
      " [22. ]\n",
      " [21.5]\n",
      " [21.5]\n",
      " [43.1]\n",
      " [36.1]\n",
      " [32.8]\n",
      " [39.4]\n",
      " [36.1]\n",
      " [19.9]\n",
      " [19.4]\n",
      " [20.2]\n",
      " [19.2]\n",
      " [20.5]\n",
      " [20.2]\n",
      " [25.1]\n",
      " [20.5]\n",
      " [19.4]\n",
      " [20.6]\n",
      " [20.8]\n",
      " [18.6]\n",
      " [18.1]\n",
      " [19.2]\n",
      " [17.7]\n",
      " [18.1]\n",
      " [17.5]\n",
      " [30. ]\n",
      " [27.5]\n",
      " [27.2]\n",
      " [30.9]\n",
      " [21.1]\n",
      " [23.2]\n",
      " [23.8]\n",
      " [23.9]\n",
      " [20.3]\n",
      " [17. ]\n",
      " [21.6]\n",
      " [16.2]\n",
      " [31.5]\n",
      " [29.5]\n",
      " [21.5]\n",
      " [19.8]\n",
      " [22.3]\n",
      " [20.2]\n",
      " [20.6]\n",
      " [17. ]\n",
      " [17.6]\n",
      " [16.5]\n",
      " [18.2]\n",
      " [16.9]\n",
      " [15.5]\n",
      " [19.2]\n",
      " [18.5]\n",
      " [31.9]\n",
      " [34.1]\n",
      " [35.7]\n",
      " [27.4]\n",
      " [25.4]\n",
      " [23. ]\n",
      " [27.2]\n",
      " [23.9]\n",
      " [34.2]\n",
      " [34.5]\n",
      " [31.8]\n",
      " [37.3]\n",
      " [28.4]\n",
      " [28.8]\n",
      " [26.8]\n",
      " [33.5]\n",
      " [41.5]\n",
      " [38.1]\n",
      " [32.1]\n",
      " [37.2]\n",
      " [28. ]\n",
      " [26.4]\n",
      " [24.3]\n",
      " [19.1]\n",
      " [34.3]\n",
      " [29.8]\n",
      " [31.3]\n",
      " [37. ]\n",
      " [32.2]\n",
      " [46.6]\n",
      " [27.9]\n",
      " [40.8]\n",
      " [44.3]\n",
      " [43.4]\n",
      " [36.4]\n",
      " [30. ]\n",
      " [44.6]\n",
      " [33.8]\n",
      " [29.8]\n",
      " [32.7]\n",
      " [23.7]\n",
      " [35. ]\n",
      " [32.4]\n",
      " [27.2]\n",
      " [26.6]\n",
      " [25.8]\n",
      " [23.5]\n",
      " [30. ]\n",
      " [39.1]\n",
      " [39. ]\n",
      " [35.1]\n",
      " [32.3]\n",
      " [37. ]\n",
      " [37.7]\n",
      " [34.1]\n",
      " [34.7]\n",
      " [34.4]\n",
      " [29.9]\n",
      " [33. ]\n",
      " [33.7]\n",
      " [32.4]\n",
      " [32.9]\n",
      " [31.6]\n",
      " [28.1]\n",
      " [30.7]\n",
      " [25.4]\n",
      " [24.2]\n",
      " [22.4]\n",
      " [26.6]\n",
      " [20.2]\n",
      " [17.6]\n",
      " [28. ]\n",
      " [27. ]\n",
      " [34. ]\n",
      " [31. ]\n",
      " [29. ]\n",
      " [27. ]\n",
      " [24. ]\n",
      " [36. ]\n",
      " [37. ]\n",
      " [31. ]\n",
      " [38. ]\n",
      " [36. ]\n",
      " [36. ]\n",
      " [36. ]\n",
      " [34. ]\n",
      " [38. ]\n",
      " [32. ]\n",
      " [38. ]\n",
      " [25. ]\n",
      " [38. ]\n",
      " [26. ]\n",
      " [22. ]\n",
      " [32. ]\n",
      " [36. ]\n",
      " [27. ]\n",
      " [27. ]\n",
      " [44. ]\n",
      " [32. ]\n",
      " [28. ]\n",
      " [31. ]]\n",
      "Regression Coefficients:  [[  4.58388525 -34.52559606  11.30133173   1.59541636 -15.68524825\n",
      "    1.49643914 -16.07459277  -1.56073878   6.97965649   0.49752305\n",
      "    6.72750518 -14.990987     1.26984398  -2.39596685  14.71422849\n",
      "   -4.11548098  26.30717675   2.62664441  -6.07113681  -2.8683943\n",
      "   -7.30805571  -0.67766814   2.87926056  -9.57023841  -1.59152114\n",
      "   11.6191628    6.83191683   8.82963216]]\n",
      "Variance explained, calculated by SK-learn method: 0.52\n",
      "Mean Squared Error, version 2: [14.4290267]\n",
      "residual sum of squares: 288.58053409172106\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "#read in dataset without carname column\n",
    "cols=[0,1,2,3,4,5,6,7]\n",
    "\n",
    "test_size=20\n",
    "df=pd.read_csv(\"auto-mpg.csv\", usecols=cols, index_col=False)\n",
    "#clean data\n",
    "df.replace('?', np.NaN,inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.horsepower = df.horsepower.astype('float')\n",
    "#print(df)\n",
    "#scale data to zero mean and unit variance\n",
    "#print(scaled_df)\n",
    "#print(scaled_df.mean(axis=0))\n",
    "#print(scaled_df.std(axis=0))\n",
    "y_col=[\"mpg\"]\n",
    "data_col=[\"cylinders\",\"displacement\",\"horsepower\",\"weight\",\"acceleration\",\"model year\",\"origin\"]\n",
    "\n",
    "\n",
    "selected_target =df[y_col]\n",
    "selected_data = df[data_col]\n",
    "#print(selected_target)\n",
    "#print(selected_data)\n",
    "poly = PolynomialFeatures(interaction_only=True,include_bias = False)\n",
    "poly_data=pd.DataFrame(poly.fit_transform(selected_data))\n",
    "# Convert data and target into dicts, T is used to transpose\n",
    "dict_data = poly_data.T.to_dict('dict').values()\n",
    "dict_target = selected_target.T.to_dict('dict').values()\n",
    "#print(dict_data)\n",
    "#print(dict_target)\n",
    "# Make new vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "\n",
    "# Transform data and target\n",
    "data = vectorizer.fit_transform(dict_data)\n",
    "target = vectorizer.fit_transform(dict_target)\n",
    "print(data)\n",
    "print(target)\n",
    "# We really need standardizing the data in this case.\n",
    "preprocessing.scale(data, copy=False)\n",
    "\n",
    "# Split data and target into train and test data\n",
    "data_train = data[:-test_size]\n",
    "data_test = data[-test_size:]\n",
    "#print(data_train)\n",
    "target_train = target[:-test_size]\n",
    "target_test = target[-test_size:]\n",
    "#print(target_train)\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(data_train, target_train)\n",
    "\n",
    "y_predicted = regr.predict(data_test)\n",
    "y_true = target_test\n",
    "\n",
    "print(\"Regression Coefficients: \", regr.coef_)\n",
    "print(\"Variance explained, calculated by SK-learn method: %.2f\" % regr.score(data_test, target_test))\n",
    "print(\"Mean Squared Error, version 2: \" + str(mean_squared_error(y_true, y_predicted, multioutput='raw_values')))\n",
    "print(\"residual sum of squares: \"+ str(((y_predicted - y_true) ** 2).sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import e\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "plt.rc(\"font\", size=14)\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "dataframe_all = pd.read_csv(\"titanic3.csv\")\n",
    "\n",
    "y=dataframe_all[\"survived\"]\n",
    "\n",
    "pd.crosstab(y,y).plot(kind='bar')\n",
    "plt.title('Survival_Distribution')\n",
    "plt.xlabel('Survived or Not')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('Survival_Distribution')\n",
    "\n",
    "\n",
    "table=pd.crosstab(dataframe_all[\"pclass\"],y)\n",
    "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
    "plt.legend(bbox_to_anchor=(1,0), loc=\"lower right\")\n",
    "plt.title('Stacked Bar PClass Type Versus Survival')\n",
    "plt.xlabel('PClass_Type')\n",
    "plt.ylabel('Proportion of Survival')\n",
    "plt.savefig('PClass_Versus_Survival')\n",
    "\n",
    "table=pd.crosstab(dataframe_all[\"sex\"],y)\n",
    "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
    "plt.legend(bbox_to_anchor=(1,0), loc=\"lower right\")\n",
    "plt.title('Stacked Bar Gender Versus Survival')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Proportion of Survival')\n",
    "plt.savefig('Gender_Versus_Survival')\n",
    "\n",
    "table=pd.crosstab(dataframe_all[\"sibsp\"],y)\n",
    "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
    "plt.legend(bbox_to_anchor=(1,0), loc=\"lower right\")\n",
    "plt.title('Stacked Bar \"Number of Siblings/Spouses Aboard\" Versus Survival')\n",
    "plt.xlabel('Number of Siblings/Spouses Aboard')\n",
    "plt.ylabel('Proportion of Survival')\n",
    "plt.savefig('Number of Siblings_Spouses Aboard Versus Survival')\n",
    "\n",
    "table=pd.crosstab(dataframe_all[\"parch\"],y)\n",
    "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
    "plt.legend(bbox_to_anchor=(1,0), loc=\"lower right\")\n",
    "plt.title('Stacked Bar \"Number of Parents/Children Aboard\" Versus Survival')\n",
    "plt.xlabel('Number of Parents/Children Aboard')\n",
    "plt.ylabel('Proportion of Survival')\n",
    "plt.savefig('Number of Number of Parents_Children Aboard Versus Survival')\n",
    "\n",
    "table=pd.crosstab(dataframe_all[\"embarked\"],y)\n",
    "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
    "plt.legend(bbox_to_anchor=(1,0), loc=\"lower right\")\n",
    "plt.title('embarked Versus Surviva')\n",
    "plt.xlabel('embarked')\n",
    "plt.ylabel('Proportion of Survival')\n",
    "plt.savefig('embarked Versus Survival')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.(a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________Continuous_Variables__________\n",
      "age mean = 28.82\n",
      "fare mean = 53.26\n",
      "sibsp mean = 0.49\n",
      "________Categorical_Variables__________\n",
      "the mode for pclass of the survivors is: 1\n",
      "the mode for sex of the survivors is: female\n",
      "the mode for embarked of the survivors is: S\n"
     ]
    }
   ],
   "source": [
    "# Question A\n",
    "import pandas as pd\n",
    "# Constant for the file name\n",
    "titanic_file = \"titanic3.csv\"\n",
    "# Read in the csv into a pandas data frame.\n",
    "# As specified by professor, We only choose the 8 columns by the problems.\n",
    "potential_columns = [0, 1, 3, 4, 5, 6, 8, 10]\n",
    "titanic_data = pd.read_csv(titanic_file, usecols=potential_columns)\n",
    "# Remove NaN values\n",
    "titanic_data.dropna(inplace=True)\n",
    "\n",
    "# Split into survivors and non-survivors\n",
    "survivors = titanic_data[titanic_data['survived'] == 1]\n",
    "non_survivors = titanic_data[titanic_data['survived'] == 0]\n",
    "\n",
    "# Continuous categories\n",
    "continuous_variables = [\"age\", \"fare\", \"sibsp\"]\n",
    "print (\"________Continuous_Variables__________\")\n",
    "\n",
    "# Print out means for survivors\n",
    "for variable in continuous_variables:\n",
    "    print(\"%s mean = %.2f\" % (variable, survivors[variable].mean()))\n",
    "\n",
    "# Categorical categories\n",
    "categorical_variables = [\"pclass\", \"sex\", \"embarked\"]\n",
    "\n",
    "print (\"________Categorical_Variables__________\")\n",
    "\n",
    "# Similarly, Please print out the modes for by yourself.\n",
    "for var in categorical_variables:\n",
    "    print(\"the mode for \"+var+\" of the survivors is: \"+str(survivors[var].mode().iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the averageage for male passengers is: 30.539695585996952\n",
      "the average age for female passengers is: 28.576658031088083\n",
      "the averagesibsp for male passengers is: 0.4368340943683409\n",
      "the average sibsp for female passengers is: 0.6191709844559585\n",
      "the averageparch for male passengers is: 0.2785388127853881\n",
      "the average parch for female passengers is: 0.6658031088082902\n",
      "the averagefare for male passengers is: 28.641018721461116\n",
      "the average fare for female passengers is: 50.15493419689128\n",
      "________Categorical_Variables__________\n",
      "Male pclass mode =  3\n",
      "Female pclass mode =  3\n",
      "Male survived mode =  0\n",
      "Female survived mode =  1\n",
      "Male embarked mode =  S\n",
      "Female embarked mode =  S\n"
     ]
    }
   ],
   "source": [
    "# Question B\n",
    "# Split into male and female passengers\n",
    "male_passengers = titanic_data[titanic_data['sex'] == 'male']\n",
    "female_passengers = titanic_data[titanic_data['sex'] == 'female']\n",
    "\n",
    "# Modify categorical variables to exclude sex and include survived\n",
    "categorical_variables = [\"pclass\", \"survived\", \"embarked\"]\n",
    "continuous_variables=[\"age\",\"sibsp\",\"parch\",\"fare\"]\n",
    "# Please Print out means by yourself\n",
    "\"\"\"print(\"the average age for male passengers is: \"+str(male_passengers[\"age\"].mean()))\n",
    "print(\"the average age for female passengers is: \"+str(female_passengers[\"age\"].mean()))\n",
    "print(\"the average sibsp for male passengers is: \"+str(male_passengers[\"sibsp\"].mean()))\n",
    "print(\"the average sibsp rate for female passengers is: \"+str(female_passengers[\"sibsp\"].mean()))\n",
    "print(\"the average parch for male passengers is: \"+str(male_passengers[\"parch\"].mean()))\n",
    "print(\"the average parch for female passengers is: \"+str(female_passengers[\"parch\"].mean()))\n",
    "print(\"the average fare for male passengers is: \"+str(male_passengers[\"fare\"].mean()))\n",
    "print(\"the average fare for female passengers is: \"+str(female_passengers[\"fare\"].mean()))\"\"\"\n",
    "\n",
    "print()\n",
    "for var in continuous_variables:\n",
    "    print(\"the average\"+var+\" for male passengers is: \"+str(male_passengers[var].mean()))\n",
    "    print(\"the average \"+var+\" for female passengers is: \"+str(female_passengers[var].mean()))\n",
    "# Print out modes for male and female passengers\n",
    "print (\"________Categorical_Variables__________\")\n",
    "\n",
    "for variable in categorical_variables:\n",
    "    print(\"Male \" + variable + \" mode = \", male_passengers[variable].mode().iloc[0])\n",
    "    print(\"Female \" + variable + \" mode = \", female_passengers[variable].mode().iloc[0])\n",
    "\n",
    "# what is iloc[0]??? Please read the panda documentation--first row of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the averageage for class 1 passengers is: 39.08304964539007\n",
      "the average age for class 2 passengers is: 29.506704980842912\n",
      "the average age for class 3 passengers is: 24.745\n",
      "the averagesibsp for class 1 passengers is: 0.4787234042553192\n",
      "the average sibsp for class 2 passengers is: 0.41762452107279696\n",
      "the average sibsp for class 3 passengers is: 0.564\n",
      "the averageparch for class 1 passengers is: 0.4148936170212766\n",
      "the average parch for class 2 passengers is: 0.39080459770114945\n",
      "the average parch for class 3 passengers is: 0.442\n",
      "the averagefare for class 1 passengers is: 92.31609148936174\n",
      "the average fare for class 2 passengers is: 21.85504444444445\n",
      "the average fare for class 3 passengers is: 12.879299000000007\n",
      "________Categorical_Variables__________\n",
      "Class 1 sex mode =  male\n",
      "Class 2 sex mode =  male\n",
      "Class 3 sex mode =  male\n",
      "Class 1 survived mode =  1\n",
      "Class 2 survived mode =  0\n",
      "Class 3 survived mode =  0\n",
      "Class 1 embarked mode =  S\n",
      "Class 2 embarked mode =  S\n",
      "Class 3 embarked mode =  S\n"
     ]
    }
   ],
   "source": [
    "# Question C is left blank, but it is very similar compared to Question B.\n",
    "# Split passengers by class\n",
    "class_1_passengers = titanic_data[titanic_data['pclass'] == 1]\n",
    "class_2_passengers = titanic_data[titanic_data['pclass'] == 2]\n",
    "class_3_passengers = titanic_data[titanic_data['pclass'] == 3]\n",
    "# Modify categorical variables to exclude sex and include survived\n",
    "categorical_variables = [\"sex\", \"survived\", \"embarked\"]\n",
    "continuous_variables=[\"age\",\"sibsp\",\"parch\",\"fare\"]\n",
    "# Please Print out means by yourself\n",
    "\"\"\"print(\"the average age for male passengers is: \"+str(_passengers[\"age\"].mean()))\n",
    "print(\"the average age for female passengers is: \"+str(female_passengers[\"age\"].mean()))\n",
    "print(\"the average sibsp for male passengers is: \"+str(male_passengers[\"sibsp\"].mean()))\n",
    "print(\"the average sibsp rate for female passengers is: \"+str(female_passengers[\"sibsp\"].mean()))\n",
    "print(\"the average parch for male passengers is: \"+str(male_passengers[\"parch\"].mean()))\n",
    "print(\"the average parch for female passengers is: \"+str(female_passengers[\"parch\"].mean()))\n",
    "print(\"the average fare for male passengers is: \"+str(male_passengers[\"fare\"].mean()))\n",
    "print(\"the average fare for female passengers is: \"+str(female_passengers[\"fare\"].mean()))\"\"\"\n",
    "\n",
    "print()\n",
    "for var in continuous_variables:\n",
    "    print(\"the average\"+var+\" for class 1 passengers is: \"+str(class_1_passengers[var].mean()))\n",
    "    print(\"the average \"+var+\" for class 2 passengers is: \"+str(class_2_passengers[var].mean()))\n",
    "    print(\"the average \"+var+\" for class 3 passengers is: \"+str(class_3_passengers[var].mean()))\n",
    "# Print out modes for male and female passengers\n",
    "print (\"________Categorical_Variables__________\")\n",
    "\n",
    "for variable in categorical_variables:\n",
    "    print(\"Class 1 \" + variable + \" mode = \", class_1_passengers[variable].mode().iloc[0])\n",
    "    print(\"Class 2 \" + variable + \" mode = \", class_2_passengers[variable].mode().iloc[0])\n",
    "    print(\"Class 3 \" + variable + \" mode = \", class_3_passengers[variable].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold scores are\n",
      "[0.67619048 0.79047619 0.88571429 0.85714286 0.80952381 0.81730769\n",
      " 0.83653846 0.65384615 0.66019417 0.7184466 ]\n",
      "Average Accuracy = 0.77\n"
     ]
    }
   ],
   "source": [
    "# Question D\n",
    "# Target column\n",
    "target_column = ['survived']\n",
    "# Data columns\n",
    "data_columns = ['pclass','sex',\"age\", \"fare\", \"sibsp\"]\n",
    "from sklearn.utils import shuffle\n",
    "shuffle(titanic_data)\n",
    "\n",
    "# Split data into data and target\n",
    "titanic_final_data = titanic_data[data_columns]\n",
    "titanic_final_target = titanic_data[target_column]\n",
    "\n",
    "# Convert data and target into dicts\n",
    "titanic_dict_data = titanic_final_data.T.to_dict('dict').values()\n",
    "titanic_dict_target = titanic_final_target.T.to_dict('dict').values()\n",
    "\n",
    "# Make new vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform data and target\n",
    "data = vectorizer.fit_transform(titanic_dict_data)\n",
    "target = vectorizer.fit_transform(titanic_dict_target)\n",
    "# Reshape target\n",
    "target = target.reshape((titanic_data.shape[0], ))\n",
    "# Create new logistic regression instance\n",
    "classifier = LogisticRegression(solver=\"lbfgs\",max_iter=1000)\n",
    "# Fit the model\n",
    "classifier.fit(data, target)\n",
    "\n",
    "\n",
    "# Cross Validation\n",
    "scores = cross_val_score(classifier, data, target, cv=10)\n",
    "print(\"10 fold scores are\")\n",
    "print(scores)\n",
    "print(\"Average Accuracy = %.2f\" % np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05082297 0.94917703]\n",
      " [0.26278591 0.73721409]\n",
      " [0.02921037 0.97078963]\n",
      " ...\n",
      " [0.88368575 0.11631425]\n",
      " [0.88562857 0.11437143]\n",
      " [0.89299401 0.10700599]]\n",
      "[0.94917703 0.73721409 0.97078963 ... 0.11631425 0.11437143 0.10700599]\n",
      "[0.9009370401329874, 0.543484610793275, 0.9424325005656718, 0.23140552418236668, 0.8697891777783838, 0.1126651844481284, 0.5436192888213542, 0.16228652866619148, 0.5425461713978641, 0.032620754730525885, 0.13053131161795853, 0.9114242861098354, 0.8917311316840768, 0.8859476854619368, 0.017096000123486207, 0.4398268693522527, 0.8105519881964715, 0.8591700805312525, 0.21920689495027929, 0.1351554130640491, 0.6899207332742996, 0.2920540632463875, 0.8494272388441296, 0.9028513008594432, 0.30015275741375214, 0.24810585598556478, 0.881734633011046, 0.8598743976755409, 0.27010460048646007, 0.13379034119566213, 0.16708811065314236, 0.8881580050544801, 0.6560113818421559, 0.15082058858215613, 0.8434915903133159, 0.8963348163596851, 0.15948169872770376, 0.12008040491329501, 0.7730439600673769, 0.4759967615934005, 0.6620415701694069, 0.8283144908031983, 0.13074740511197402, 0.15072278648852674, 0.7018903823166851, 0.4629038021513307, 0.8448390843318652, 0.21281653912328374, 0.28097095215539375, 0.3989817281887488, 0.41213494594551864, 0.9059139010258618, 0.1672884231704162, 0.8014028306733143, 0.10691197726342216, 0.15138317411132024, 0.39882269223392774, 0.08745919247760522, 0.6939731186174164, 0.21157178817136846, 0.7978694528156064, 0.8848908828461783, 0.870873034883006, 0.1318126811281098, 0.2521435465858007, 0.8616107215991264, 0.9134910377470091, 0.1181721489484692, 0.7676363088109748, 0.14601561946556854, 0.6268695122279521, 0.5054555849948119, 0.021481864531120363, 0.837359229997118, 0.5025518160176893, 0.12871574857982906, 0.7703044958069077, 0.09652047103930157, 0.2821738177931445, 0.8728547024828754, 0.1779928887840004, 0.8335195051274099, 0.1800790286750746, 0.882520219957103, 0.06307722742497399, 0.5641715514101945, 0.639057385257464, 0.08020557327945073, 0.8829494572959184, 0.7061626608571562, 0.6746694107616731, 0.0733388607158965, 0.1741890723615471, 0.8981149428503272, 0.8685520895384419, 0.6371947956558024, 0.8257249358609621, 0.19688914519544667, 0.25129333865270315, 0.8150534113491291, 0.7895963085656268, 0.8210059563456474, 0.22706753799830953, 0.05823298428567102, 0.6732869859922157, 0.8637563868795332, 0.053194686414098025, 0.09335882404624878, 0.8952693522897223, 0.041421966709381676, 0.6935854221690091, 0.13534395389339005, 0.7845559763217843, 0.12226859424150477, 0.8779183028119564, 0.8971800565705538, 0.7778415417933977, 0.3405903624520772, 0.08056205836295927, 0.03105193020056018, 0.08727627748968436, 0.9022592888651457, 0.23874276769027364, 0.7162526798791399, 0.3423169077453237, 0.7793060351673321, 0.14250087809694692, 0.23051219649758922, 0.8452114984215827, 0.08208330626174659, 0.6834900135436102, 0.0981797517393995, 0.7953339721815585, 0.15501710082425835, 0.307325537361726, 0.8944788654789348, 0.05826457198972491, 0.6640920641128951, 0.15684446194092194, 0.08332339298079998, 0.9166276367661952, 0.7842344206440784, 0.6657337788842699, 0.1051960884192238, 0.7841573743558882, 0.2054100906321174, 0.1416832081129843, 0.7975919397485218, 0.7276822064807736, 0.09955518773534419, 0.1243217784229504, 0.1015376226053721, 0.3246951252834817, 0.06648822250299746, 0.11067474350451977, 0.10535530932474227, 0.7073853447327559, 0.8599481843873957, 0.7345613180735284, 0.8754804454084325, 0.47416192131487417, 0.15082058858215613, 0.6843426985145825, 0.9136489057333996, 0.7241384230799636, 0.262088486205927, 0.9041160617348929, 0.26035859166598546, 0.7131541920065514, 0.9400061445634034, 0.25040814144299894, 0.9210145340817402, 0.28727753616529267, 0.8775132588765658, 0.887693437417169, 0.1410873063642382, 0.08840220094125484, 0.19684490264013765, 0.21631801630836409, 0.04374172956843279, 0.06577803408810536, 0.8102481515138856, 0.78426397137412, 0.1884057831309582, 0.07875772181672917, 0.12352605466581419, 0.19008566844336178, 0.8293329365686869, 0.8696544086427979, 0.08494389566701328, 0.9015780506771047, 0.04636525977111499, 0.8319231176287991, 0.897672590510771, 0.048941967335250705, 0.2657940836293029, 0.12824373303525385, 0.3597334590232877, 0.20057008098078832, 0.8629125124834388, 0.32857264318134966, 0.8932579861905703, 0.8725086915065403, 0.09257891795153088, 0.12666143033107471, 0.701500877211382, 0.1697783624219723, 0.3955809391087758, 0.8397477074557254, 0.25326798914030846, 0.13074740511197402, 0.8411363180749455, 0.14250087809694692, 0.2030235439021813, 0.8569621964434923, 0.052438000832740236, 0.6273236401709062, 0.222929348842939, 0.3770364753053428, 0.8873130226155519, 0.8749804927233749, 0.06895831312273094, 0.7712780919946933, 0.8917311316840768, 0.786234551990118, 0.8572342796528043, 0.21406766501318844, 0.8393740080585733, 0.20529100913735288, 0.0691450289512686, 0.7569857208337251, 0.0756381966594889, 0.27481353948609477, 0.07367652865536493, 0.07458000530260127, 0.24199421526338005, 0.8790514850645624, 0.25318241777439543, 0.8621211253388029, 0.5726107055667118, 0.11284633491186954, 0.779857712061356, 0.06105361953768648, 0.23391323436771558, 0.0522894349274842, 0.05491920544236674, 0.7254248089211387, 0.6564767991715887, 0.04285701792092555, 0.623466926586272, 0.056434558300346625, 0.7424708053604556, 0.9140625422341092, 0.06632049981102844, 0.7662858687636269, 0.07639748322896162, 0.08576764928697024, 0.4370344116082559, 0.7780977530439411, 0.24223108800982712, 0.16578279540894578, 0.05663984465985375, 0.12083596293322374, 0.9305669004125257, 0.03195805906065929, 0.5742960238488273, 0.05874751355666338, 0.09496557713542533, 0.37228622848114773, 0.7331104728002317, 0.8842013408486936, 0.06454069048811373, 0.7553318100650349, 0.10770974133343973, 0.3846962723643892, 0.735750223045047, 0.8944692003587738, 0.10623724720198714, 0.36298553409837325, 0.8775628686193145, 0.0522894349274842, 0.855041577545955, 0.039390257708880466, 0.5888376777940165, 0.06359417604518543, 0.12027805448088034, 0.0833518732834157, 0.030951367888359005, 0.5033571323164922, 0.0115836871762408, 0.12027805448088034, 0.09277838297304129, 0.5899914967528231, 0.07066250370192982, 0.01729462565814197, 0.03506092799771727, 0.6781462027137276, 0.07397256116546562, 0.12680859884248927, 0.7377977030022427, 0.6689063733772874, 0.6042438755934791, 0.05042243883381515, 0.7457073304244141, 0.09343177013557515, 0.07950806847289534, 0.03096026011603819, 0.07525484945733127, 0.7047877569891826, 0.7848813939434519, 0.005669853634908158, 0.4662482002569035, 0.6689488400070931, 0.05346213274454429, 0.5899914967528231, 0.08395235856274832, 0.03096026011603819, 0.5255611198915171, 0.2642204888578595, 0.05088447410841646, 0.6516011211428302, 0.6047073746158038, 0.11486287888518507, 0.41338650168867086, 0.00821650706644453, 0.016240746240761085, 0.02559210586294216, 0.5794640668944985, 0.6227584199630274, 0.5023131912175827, 0.042129816338264806, 0.5899438040433275, 0.06680097889978674, 0.07118804047454633, 0.08796551864247691, 0.8246512173974015, 0.03732190647123613, 0.5583130534523382, 0.720576003481684, 0.6501206903506157, 0.1033004267663419, 0.13687624865673745, 0.14114825560177077, 0.47271815827155883, 0.6689913032790585, 0.05668622803104081, 0.14797118100182857, 0.04236313424969512, 0.6314954020755752, 0.0885034852832089, 0.14114825560177077, 0.7573639074589733, 0.6159544563847558, 0.014855781479442656, 0.20045924832491593, 0.018923371604335132, 0.5293021536155122, 0.5936455987326499, 0.5619912306290195, 0.09343177013557515, 0.10372587017807343, 0.1207548181765886, 0.02108669317100499, 0.5794640668944985, 0.1199609810201614, 0.04476028530165108, 0.5689192631726615, 0.04950515012722964, 0.02359489205573053, 0.030439990008882922, 0.6105812741952348, 0.13769186221467014, 0.07893318136042307, 0.022315273619143837, 0.06441829436578807, 0.06441829436578807, 0.08871793499997817, 0.08859218122206865, 0.05042243883381515, 0.06359417604518543, 0.01621214579718737, 0.06359417604518543, 0.1814813116448237, 0.7054726058708367, 0.027285516290201216, 0.8378237559079541, 0.07550823956178206, 0.00814395776927833, 0.06311538381617522, 0.8303437538907644, 0.017333421621529656, 0.4023107966119703, 0.6507347684079977, 0.6507347684079977, 0.013320110978194018, 0.39174550247296014, 0.3816675020791261, 0.03922573579118551, 0.0237330312560009, 0.047013964895853175, 0.6809684774007014, 0.5810092782436193, 0.03406824898503632, 0.04476028530165108, 0.3026793829940759, 0.018504838768449376, 0.016207214720597555, 0.5794640668944985, 0.12201404441005936, 0.03096026011603819, 0.0043666643378168085, 0.24475408517223382, 0.05335617241002322, 0.759875711990291, 0.018563938494960464, 0.5404006100428106, 0.022662784895255788, 0.034197187498305255, 0.0386833762955683, 0.05625144986891418, 0.7136712601280725, 0.030951367888359005, 0.6305934054437103, 0.7251685702690609, 0.047421335481243033, 0.4925417609930316, 0.011556573770787174, 0.05960060786895734, 0.610534745215619, 0.039646073113356245, 0.8195040804098643, 0.8073531390428993, 0.05610252434155893, 0.6579502013792822, 0.7604658895062534, 0.6092320878119303, 0.04474246770908161, 0.08796551864247691, 0.008786519457711292, 0.01081769346332881, 0.435915353732968, 0.35639096835270667, 0.26671545423110743, 0.038650434820406386, 0.6363406623298864, 0.06359417604518543, 0.035417808020654314, 0.05677352310982351, 0.06359417604518543, 0.02509569811692535, 0.7913864906623824, 0.5402962792581639, 0.11409789136261438, 0.037301996638085436, 0.021092977178815402, 0.004645750941130583, 0.07525484945733127, 0.014235773711306231, 0.038912392254023626, 0.13234455691535255, 0.008119673584654362, 0.03444917447856578, 0.7234185808892049, 0.17582560405039352, 0.16815974745143109, 0.04527225923301609, 0.07950806847289534, 0.07763737046831219, 0.07129353063140753, 0.10944240468913032, 0.6583817824635629, 0.037301996638085436, 0.09779447914815226, 0.09277838297304129, 0.06747581231346228, 0.07066250370192982, 0.44374649249687964, 0.11409789136261438, 0.03295862641184416, 0.7332797418771833, 0.7510098345540138, 0.029859590651716186, 0.6306386643520434, 0.05042243883381515, 0.06354376136182462, 0.07570450777151942, 0.8084341154871204, 0.7681404131825502, 0.6275619327432682, 0.044405098580927206, 0.030439990008882922, 0.37261045697935, 0.6689913032790585, 0.09396911524667274, 0.1820008669791737, 0.16527068138145432, 0.5356219382620325, 0.43447125994096275, 0.11409789136261438, 0.7287837103954038, 0.07950806847289534, 0.08395235856274832, 0.07816740090151005, 0.7021051891025092, 0.7533701283518011, 0.7477144753513614, 0.6501206903506157, 0.010124309188934656, 0.649789932035075, 0.047144803039445245, 0.5474213497266828, 0.08395235856274832, 0.033182235199446665, 0.0833518732834157, 0.11778163341878331, 0.16637354081752184, 0.43447125994096275, 0.09343177013557515, 0.6688190454833849, 0.6769777379374844, 0.041458221784315505, 0.597567660555737, 0.03507086254450337, 0.6445743463579268, 0.039009246605833964, 0.057258614655479816, 0.6445743463579268, 0.7966628919335197, 0.549002302909819, 0.6256766974677281, 0.04750587440391959, 0.5794640668944985, 0.1745910180910188, 0.794636762176413, 0.6646172497023674, 0.815605325633049, 0.7896739264062611, 0.02743899742380151, 0.537496668613038, 0.00616134303737195, 0.06005704603878272, 0.6874585604999859, 0.7047877569891826, 0.0046276483701282655, 0.033970684855289035, 0.01598934235851414, 0.21556407158488666, 0.5013767709277642, 0.014983607905442549, 0.02086381549700936, 0.4787394500330212, 0.010671765040158411, 0.014032670227986295, 0.16983321003591034, 0.06759891927140758, 0.48000290876728624, 0.014578930057325249, 0.01402455175173932, 0.02086381549700936, 0.01598292735912422, 0.014951766905711325, 0.007582852000146406, 0.023781049809878944, 0.009837078070319928, 0.3703171774645049, 0.006348626253931994, 0.24757753200188484, 0.28685892163798093, 0.146882300789082, 0.04764253802738408, 0.2200084921649681, 0.2026232301610157, 0.0033719006279258213, 0.013107562710920314, 0.014018933835026677, 0.1867413197926015, 0.020858710646250635, 0.014024966312325268, 0.00860422756467326, 0.38726151366603834, 0.01602386911624926, 0.007554935637031552, 0.005920728850144479, 0.00446234998587859, 0.006811695955797273, 0.0033534685635603968, 0.2571992358110249, 0.003139577039375519, 0.01712451854071181, 0.1949749634222977, 0.1984137377600092, 0.019496703726900984, 0.017079730691797777, 0.49424479218927836, 0.010670798636262618, 0.017128030242345927, 0.534293699264373, 0.020813394026663552, 0.005236765858309028, 0.10461834779934652, 0.4843496879721362, 0.4843496879721362, 0.4364765529169315, 0.41831294445692424, 0.4792284880211649, 0.005325022711569781, 0.014024966312325268, 0.020813696222471437, 0.4830419481083177, 0.2016427054156727, 0.3780126070824462, 0.018309642273239907, 0.022274125614546062, 0.014018933835026677, 0.01839296183309726, 0.02058342364738942, 0.011126760770795325, 0.019534006928345168, 0.023736397833417282, 0.014024966312325268, 0.030052269114371505, 0.48694282355973545, 0.002954852207393668, 0.24048338263067157, 0.020106705568422603, 0.4339555377419953, 0.4565955372525239, 0.006258993646921339, 0.010177027086294517, 0.018293390813760718, 0.007582852000146406, 0.4731511716452182, 0.01953711622349017, 0.022174809602745066, 0.47912672936957124, 0.44579834970949833, 0.34554053415097696, 0.023810521246293886, 0.006163073625088523, 0.025389762058006256, 0.025389762058006256, 0.44525223615473924, 0.019532309906657927, 0.01955268310952792, 0.01225092912467761, 0.016028347539272274, 0.5014358969921583, 0.27209955662690605, 0.01222692890227272, 0.01603942417943041, 0.019531176371634568, 0.011126760770795325, 0.011457513391375695, 0.007930587223135362, 0.013702425935440486, 0.020800806105461426, 0.023760744001467292, 0.015994211934272293, 0.007058235984471048, 0.01600809080011692, 0.00998348384779623, 0.009984090094290821, 0.4339704935591185, 0.34494616889479623, 0.0005795046812641141, 0.004318311711048738, 0.007578540900067038, 0.013112252407513717, 0.02226232403083002, 0.010702733549304426, 0.024889960788219607, 0.036307931858762146, 0.28646303107977883, 0.001352627542209965, 0.022282417687118663, 0.495218058986236, 0.004145659133312001, 0.025389762058006256, 0.01766469649267952, 0.0037441800613023575, 0.4356257287557115, 0.022437019520689893, 0.34455564645341086, 0.011445120328207473, 0.0705769238349153, 0.004520214674266849, 0.27866388179577434, 0.013112252407513717, 0.014996670306588139, 0.0051436430818566975, 0.018309642273239907, 0.006353220700300581, 0.007912002915095214, 0.0038258304363557242, 0.2055752793709344, 0.010759079922175698, 0.027139827834131765, 0.04165709804060461, 0.586223506465997, 0.008111146467165946, 0.23371273562140157, 0.014996670306588139, 0.018258541761475264, 0.007051818946638554, 0.46787976095869777, 0.025323731090294174, 0.004647070469046127, 0.004315799175823717, 0.009321394272673715, 0.022274125614546062, 0.34773605065218377, 0.4114486428032408, 0.42287816418985136, 0.008705131283464087, 0.0008712041489597513, 0.01601115265499159, 0.009735258130207584, 0.34103330274572724, 0.023738425814396086, 0.02698227229763242, 0.0037289450557123354, 0.0057141033075822825, 0.014281873911027396, 0.01632158519249946, 0.0032343444470707707, 0.6214173600562592, 0.005294168883118103, 0.005150322748341883, 0.023740113214591723, 0.014024137202584242, 0.2723375014503604, 0.4004994595140505, 0.00813245305608249, 0.01680502753156317, 0.12400862252524791, 0.014988479825003051, 0.018309642273239907, 0.5014260230426713, 0.043435340553692824, 0.004965522652784258, 0.00497319724943764, 0.2524535159429961, 0.006126184509029214, 0.0025372699997458555, 0.004503345706743032, 0.0021949668993593973, 0.1494726438268275, 0.11157212367376262, 0.0017644227588161274, 0.0033307288317484664, 0.16222977245097428, 0.0024312071743590505, 0.009342253956416555, 0.021002747005247655, 0.0018999427914772496, 0.0036398978120759412, 0.0222530826768492, 0.4121042731743954, 0.4902543333547475, 0.006935670096778167, 0.3207351815032166, 0.020977672947626064, 0.01653332819418562, 0.001452905241872106, 0.007740177957009751, 0.019539383987439336, 0.1361965944857807, 0.014962373571319694, 0.03824867352779012, 0.013069128927527906, 0.47845391110661933, 0.389159827228358, 0.4228034874381109, 0.43469587169282115, 0.0122553311664176, 0.3668972417666885, 0.6507310671576496, 0.34010947720602763, 0.0042923098180994235, 0.01291553023143637, 0.3780395463763055, 0.004629390972742289, 0.010670960986352966, 0.28527632336313996, 0.3058158662140711, 0.011451141782070356, 0.01953541895362685, 0.020858710646250635, 0.0030168884397433113, 0.01427298642189542, 0.008081593092832101, 0.014018933835026677, 0.018293390813760718, 0.008729345182581389, 0.009984997713911072, 0.011449423423070551, 0.03359751828713392, 0.5721971211254383, 0.0027240306096166502, 0.008701280315177612, 0.02166559796615521, 0.37992760482893806, 0.017130532134553778, 0.0093249398582927, 0.013110301303122048, 0.3604612264003212, 0.34953378435253807, 0.009327349679338431, 0.025257503779811473, 0.019578741678477812, 0.010696757788505352, 0.01953541895362685, 0.008703804939731951, 0.01827582454700917, 0.631859429220713, 0.005847609630256932, 0.022941870881741735, 0.007844496440446049, 0.004021358253518068, 0.2493697475895836, 0.004210320555773117, 0.6363510780932157, 0.0019073984625089648, 0.3031954579560109, 0.5727881077907334, 0.013383373980721636, 0.28446693533167233, 0.014961270152039966, 0.2730665498576966, 0.4336713806526011, 0.016657972397425894, 0.011516897619123959, 0.011446152601952848, 0.018292061621333722, 0.018256946990972772, 0.009327349679338431, 0.007804397592920648, 0.0068650297285209804, 0.006086321207154909, 0.01612167363403164, 0.4002455734098635, 0.1986471861423151, 0.003937224321191419, 0.25954377823217467, 0.011692935361548354, 0.014579049820447812, 0.006034771169646095, 0.29994116149220584, 0.020146929201677623, 0.013148260488308727, 0.002421926460920428, 0.42276119093806147, 0.009315585266649695, 0.015988100419684792, 0.01825056919300843, 0.011452348082717454, 0.3396069413006135, 0.007578540900067038, 0.008705131283464087, 0.5127189965133359, 0.29224401888858825, 0.00916157457139527, 0.37515299393877566, 0.0017528649359199616, 0.019534006928345168, 0.016030700307284897, 0.019542208845999547, 0.0122553311664176, 0.014983607905442549, 0.05064207917797805, 0.3807177351024568, 0.008127898298715769, 0.4114486428032408, 0.023736397833417282, 0.018299782405014346, 0.5122459514669095, 0.6619592651051067, 0.01202663417203957, 0.3749425869469299, 0.008710049355184424, 0.020170120718281108, 0.42823225434390483, 0.3560068722514164, 0.01226212202197277, 0.4790646481220863, 0.389117957418673, 0.019539383987439336, 0.004959676507460566, 0.005728969283405726, 0.01953570408549585, 0.011818432983773875, 0.4339704935591185, 0.001163202064226715, 0.017210650516852583, 0.43398544939414574, 0.04108225604913045, 0.01291553023143637, 0.004642554062398956, 0.33428346044383406, 0.012253499736903602, 0.009322244824636285, 0.02095775218071354, 0.42324409360214094, 0.457079010487782, 0.02091705908166055, 0.027113895406003816, 0.33475585936359137, 0.012777355441573412, 0.009730134223465893, 0.3627501697700387, 0.30904199487298845, 0.36365548859130375, 0.008072670220443381, 0.005304527126697064, 0.007531154183105207, 0.002789399364986022, 0.00322137324101621, 0.24969191704147267, 0.01959433903058316, 0.02277291215875438, 0.009326355726926324, 0.04137841946750861, 0.5522755927903263, 0.39262201232527194, 0.019544191903524792, 0.014998222768331997, 0.018258541761475264, 0.008293543618670551, 0.01603942417943041, 0.3669678379398926, 0.02226232403083002, 0.008293543618670551, 0.4790646481220863, 0.009331605746058501, 0.025389762058006256, 0.01607411989463735, 0.006145507408260838, 0.01955268310952792, 0.004120440829676006, 0.006298072447302653, 0.00509860940975824, 0.007243072189558446, 0.0047500226883162855, 0.2635747040831417, 0.4530822817552668, 0.007557025964424503, 0.0014228683235049686, 0.12367725219791419, 0.5666799669436023, 0.013986576180402773, 0.23996175793329017, 0.0026135918051726987, 0.02700733446140188, 0.014961047355410461, 0.005919143092837813, 0.0002691490416201165, 0.01611172626300728, 0.44519238796046867, 0.005728969283405726, 0.5753264266446491, 0.41678609420917395, 0.5431153071670316, 0.015082181833662231, 0.020872831091135264, 0.015514138973425018, 0.011517591232481111, 0.01825721490344241, 0.005338415991591987, 0.019544191903524792, 0.4788982988245948, 0.01143456011335958, 0.007567321939822422, 0.30240075899345414, 0.37778612463429423, 0.0030980913083753434, 0.14104399314808894, 0.02224352370217475, 0.010702733549304426, 0.009331605746058501, 0.008729610534800196, 0.4225794627730168, 0.01955268310952792, 0.02226232403083002, 0.4352193649596962, 0.009990445068337628, 0.013148260488308727, 0.6498028812023198, 0.26666978545580816, 0.02700733446140188, 0.004019456794546632, 0.014951766905711325, 0.00044662697724664725, 0.03076887749856806, 0.016025045155357766, 0.014991354789259411, 0.008127898298715769, 0.06900444918750712, 0.40411387219151623, 0.009327349679338431, 0.010343150662248347, 0.004021358253518068, 0.014582321959708936, 0.02821273659819902, 0.48694282355973545, 0.3602599544422862, 0.007068690594731341, 0.48029577711903576, 0.0835728953740958, 0.021075004740157142, 0.005282283102528561, 0.5762707892538323, 0.004067465768998675, 0.263985058057421, 0.008756424899918413, 0.01232615178034939, 0.01232615178034939, 0.0032559793970510474, 0.29383516406265514, 0.0016229254799484817, 0.008794351114951507, 0.25117386769500405, 0.018256946990972772, 0.020858710646250635, 0.5236721969246683, 0.018299782405014346, 0.018370495336287502, 0.009063670427127666, 0.2620003782746502, 0.0024284075772701675, 0.01331857462101576, 0.010867203909672093, 0.12126522790551561, 0.012201626751339232, 0.019498397905992344, 0.013148260488308727, 0.007110767438406408, 0.007398161570282977, 0.4188875717691788, 0.0036013135132063434, 0.42451793964753226, 0.013529004821351439, 0.013080823944988842, 0.011450282572267217]\n",
      "424.9988102272288\n",
      "267.1060324900617\n",
      "AIC is: 861.9976204544575\n",
      "BIC is: 30.523847320009118\n",
      "the probability of survival is:0.6877022653721683\n"
     ]
    }
   ],
   "source": [
    "# Question E and F, \n",
    "# Hint: In order to calculate AIC and BIC, you need classifier.predict_log_proba\n",
    "# Hint: You also need to find the K and N used in the final model.\n",
    "probability_estimation_matrix=classifier.predict_proba(data)\n",
    "print(classifier.predict_proba(data))\n",
    "result_matrix=probability_estimation_matrix.dot([0,1])\n",
    "print(result_matrix)\n",
    "result_matrix2=[i**2 for i in result_matrix]\n",
    "print(result_matrix2)\n",
    "sum1=np.sum(result_matrix,axis=0)\n",
    "sum2=np.sum(result_matrix2,axis=0)\n",
    "print(sum1)\n",
    "print(sum2)\n",
    "#row_sum=np.sum(probability_estimation_matrix,axis=1).reshape(-1,1)\n",
    "#print(row_sum)\n",
    "k = len(data_columns)+1\n",
    "n = titanic_data.shape[0]\n",
    "#print(k)\n",
    "#print(n)\n",
    "AIC=2*k+2*sum1\n",
    "BIC=np.log((n**k)/(sum2)**2)\n",
    "print(\"AIC is: \"+str(AIC))\n",
    "print(\"BIC is: \"+ str(BIC))\n",
    "survived_len=len(titanic_data[titanic_data['survived']==1])\n",
    "non_survived_len=len(titanic_data[titanic_data['survived']==0])\n",
    "ratio=survived_len/non_survived_len\n",
    "print(\"the probability of survival is:\"+str(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01202229]]\n",
      "10 fold scores are\n",
      "[0.6        0.65714286 0.73333333 0.80952381 0.63809524 0.59615385\n",
      " 0.59615385 0.59615385 0.58252427 0.59223301]\n",
      "Average Accuracy = 0.64\n",
      "[[0.14845021 0.85154979]\n",
      " [0.26346701 0.73653299]\n",
      " [0.26346701 0.73653299]\n",
      " ...\n",
      " [0.66975801 0.33024199]\n",
      " [0.66975801 0.33024199]\n",
      " [0.6680273  0.3319727 ]]\n",
      "[0.85154979 0.73653299 0.73653299 ... 0.33024199 0.33024199 0.3319727 ]\n",
      "424.99947249037496\n",
      "AIC is: 853.9989449807499\n",
      "BIC is: 1.7955370545524871\n"
     ]
    }
   ],
   "source": [
    "target_column = ['survived']\n",
    "# Data columns\n",
    "data_columns = ['fare']\n",
    "\"\"\"\n",
    "pclass:\n",
    "AIC is: 854.0001092877758\n",
    "BIC is: 1.79553431500502\n",
    "sex:\n",
    "AIC is: 854.0005997506236\n",
    "BIC is: 1.7955331609752714\n",
    "age:\n",
    "AIC is: 854.0000953078326\n",
    "BIC is: 1.7955343478990002\n",
    "fare:\n",
    "AIC is: 853.9989449807499\n",
    "BIC is: 1.7955370545524871\n",
    "sibsp:\n",
    "AIC is: 854.0004485883072\n",
    "BIC is: 1.7955335166510904\n",
    "\n",
    "\"\"\"\n",
    "#'pclass','sex',\"age\", \"fare\", \"sibsp\"\n",
    "from sklearn.utils import shuffle\n",
    "shuffle(titanic_data)\n",
    "\n",
    "# Split data into data and target\n",
    "titanic_final_data = titanic_data[data_columns]\n",
    "titanic_final_target = titanic_data[target_column]\n",
    "\n",
    "# Convert data and target into dicts\n",
    "titanic_dict_data = titanic_final_data.T.to_dict('dict').values()\n",
    "titanic_dict_target = titanic_final_target.T.to_dict('dict').values()\n",
    "\n",
    "# Make new vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform data and target\n",
    "data = vectorizer.fit_transform(titanic_dict_data)\n",
    "target = vectorizer.fit_transform(titanic_dict_target)\n",
    "# Reshape target\n",
    "target = target.reshape((titanic_data.shape[0], ))\n",
    "classifier2 = LogisticRegression(solver=\"lbfgs\",max_iter=1000)\n",
    "# Fit the model\n",
    "classifier2.fit(data, target)\n",
    "print(classifier2.coef_)\n",
    "\n",
    "# Cross Validation\n",
    "scores = cross_val_score(classifier2, data, target, cv=10)\n",
    "print(\"10 fold scores are\")\n",
    "print(scores)\n",
    "print(\"Average Accuracy = %.2f\" % np.mean(scores))\n",
    "\n",
    "\n",
    "probability_estimation_matrix=classifier2.predict_proba(data)\n",
    "print(classifier2.predict_proba(data))\n",
    "result_matrix=probability_estimation_matrix.dot([0,1])\n",
    "print(result_matrix)\n",
    "sum=np.sum(result_matrix,axis=0)\n",
    "print(sum)\n",
    "#row_sum=np.sum(probability_estimation_matrix,axis=1).reshape(-1,1)\n",
    "#print(row_sum)\n",
    "k = len(data_columns)+1\n",
    "n = titanic_data.shape[0]\n",
    "#print(k)\n",
    "#print(n)\n",
    "AIC=2*k+2*sum\n",
    "BIC=np.log(n**k/sum**2)\n",
    "print(\"AIC is: \"+str(AIC))\n",
    "print(\"BIC is: \"+ str(BIC))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     area SE\n",
      "0     74.080\n",
      "1     94.030\n",
      "2     27.230\n",
      "3     94.440\n",
      "4     27.190\n",
      "5     53.910\n",
      "6     50.960\n",
      "7     24.320\n",
      "8     23.940\n",
      "9     40.510\n",
      "10    54.160\n",
      "11   116.200\n",
      "12    36.580\n",
      "13    19.210\n",
      "14    32.550\n",
      "15    45.400\n",
      "16    54.180\n",
      "17   112.400\n",
      "18    23.560\n",
      "19    14.670\n",
      "20    15.700\n",
      "21    44.910\n",
      "22    93.990\n",
      "23   102.600\n",
      "24   111.400\n",
      "25    21.050\n",
      "26    93.540\n",
      "27    43.500\n",
      "28    61.100\n",
      "29   105.000\n",
      "..       ...\n",
      "538   11.730\n",
      "539   20.860\n",
      "540   23.110\n",
      "541   27.410\n",
      "542   17.850\n",
      "543   23.120\n",
      "544   31.240\n",
      "545   12.970\n",
      "546    7.326\n",
      "547   18.240\n",
      "548   33.000\n",
      "549   20.670\n",
      "550   17.850\n",
      "551   18.760\n",
      "552   17.860\n",
      "553   16.830\n",
      "554   14.460\n",
      "555   16.800\n",
      "556   29.110\n",
      "557   19.540\n",
      "558   16.970\n",
      "559   29.840\n",
      "560   22.810\n",
      "561   22.650\n",
      "562  118.800\n",
      "563  158.700\n",
      "564   99.040\n",
      "565   48.550\n",
      "566   86.220\n",
      "567   19.150\n",
      "\n",
      "[568 rows x 1 columns]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Regression block 2 Coefficients:  [[0.00574801]]\n",
      "Regression block 2 Intercept:  [0.14363445]\n",
      "Variance explained, calculated by SK-learn method: 0.50\n",
      "Mean Squared Error: 0.1\n",
      "Mean Squared Error, version 2: [0.1]\n",
      "10 fold scores are\n",
      "[-0.94628634  0.38494262  0.39274662 -0.24952661  0.31663658  0.37934985\n",
      "  0.4161268   0.25558077 -0.4066801   0.39744836]\n",
      "Average Accuracy = 0.09\n",
      "     Diagnosis\n",
      "0            1\n",
      "1            1\n",
      "2            1\n",
      "3            1\n",
      "4            1\n",
      "5            1\n",
      "6            1\n",
      "7            1\n",
      "8            1\n",
      "9            1\n",
      "10           1\n",
      "11           1\n",
      "12           1\n",
      "13           1\n",
      "14           1\n",
      "15           1\n",
      "16           1\n",
      "17           1\n",
      "18           0\n",
      "19           0\n",
      "20           0\n",
      "21           1\n",
      "22           1\n",
      "23           1\n",
      "24           1\n",
      "25           1\n",
      "26           1\n",
      "27           1\n",
      "28           1\n",
      "29           1\n",
      "..         ...\n",
      "538          0\n",
      "539          0\n",
      "540          0\n",
      "541          0\n",
      "542          0\n",
      "543          0\n",
      "544          0\n",
      "545          0\n",
      "546          0\n",
      "547          0\n",
      "548          0\n",
      "549          0\n",
      "550          0\n",
      "551          0\n",
      "552          0\n",
      "553          0\n",
      "554          0\n",
      "555          0\n",
      "556          0\n",
      "557          0\n",
      "558          0\n",
      "559          0\n",
      "560          0\n",
      "561          1\n",
      "562          1\n",
      "563          1\n",
      "564          1\n",
      "565          1\n",
      "566          1\n",
      "567          0\n",
      "\n",
      "[568 rows x 1 columns]\n",
      "357\n",
      "the percentage of benign in the dataset is:0.6285211267605634\n",
      "10 fold scores are\n",
      "[0.86206897 0.78947368 0.84210526 0.85964912 0.85964912 0.87719298\n",
      " 0.85964912 0.92857143 0.89285714 0.89285714]\n",
      "Average Accuracy = 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import e\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "plt.rc(\"font\", size=14)\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "cols=[\"ID number\",\"Diagnosis\",\"Mean radius\", \"Mean texture\",\"Mean perimeter\", \"Mean area\", \"Mean smoothness\",\"Mean compactness\",\"Mean concavity\",\"Mean concave points\",\"Mean symmetry\",\"Mean fractal dimension\",\"radius SE\",\"texture SE\",\"perimeter SE\",\"area SE\",\"smoothness SE\",\"compactness SE\",\"concavity SE\",\"concave points SE\",\"symmetry SE\",\"fractal dimension SE\",\"Worst radius\",\"Worst texture\",\"Worst perimeter\",\"Worst area\",\"Worst smoothness\",\"Worst compactness\",\"Worst concavity\",\"Worst concave points\",\"Worst symmetry\",\"Worst fractal dimension\"]\n",
    "dataframe_all = pd.read_csv(\"wdbc.data\",index_col=False)\n",
    "\n",
    "dataframe_all.columns=cols\n",
    "#print(dataframe_all)\n",
    "\n",
    "\n",
    "dataframe_all.dropna(inplace=True)\n",
    "#extract data from dataframe with whether a given sample is benign or malignant as dependent variable\n",
    "#and the Standard Error of the area as the independent variable\n",
    "\n",
    "m_data=dataframe_all.iloc[:,1]\n",
    "y_data=pd.DataFrame(m_data.replace(['B','M'],[0,1]),columns=[\"Diagnosis\"])\n",
    "x_data=pd.DataFrame(dataframe_all.iloc[:,15].astype(\"float\"),columns=[\"area SE\"])\n",
    "#print(y_data)\n",
    "print(x_data)\n",
    "#print(type(y_data))\n",
    "#print(type(x_data))\n",
    "test_size=20\n",
    "k_b2 = 2\n",
    "n_b2 = y_data.shape[0]\n",
    "\n",
    "\n",
    "# Convert data and target into dicts so that we can use the DictVectorizer\n",
    "dict_data_b2 = x_data.T.to_dict('dict').values()\n",
    "dict_target_b2 = y_data.T.to_dict('dict').values()\n",
    "\n",
    "# Call a new Dictvectorizer\n",
    "vectorizer_b2 = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform data and target\n",
    "data_b2 = vectorizer_b2.fit_transform(dict_data_b2)\n",
    "target_b2 = vectorizer_b2.fit_transform(dict_target_b2)\n",
    "\n",
    "# Please read https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html\n",
    "# Uncomment the next line will standardize the data, but not the target. \n",
    "preprocessing.scale(data, copy=False)\n",
    "\n",
    "# Split data and target into train and test data\n",
    "data_train_b2 = data_b2[:-test_size]\n",
    "data_test_b2 = data_b2[-test_size:]\n",
    "\n",
    "target_train_b2 = target_b2[:-test_size]\n",
    "target_test_b2 = target_b2[-test_size:]\n",
    "\n",
    "regr_block2 = linear_model.LinearRegression()\n",
    "regr_block2.fit(data_train_b2, target_train_b2)\n",
    "\n",
    "y_predicted_block2 = regr_block2.predict(data_test_b2)\n",
    "y_true_block2 = target_test_b2\n",
    "\n",
    "y_predicted_block2[y_predicted_block2>=0.5]=1\n",
    "y_predicted_block2[y_predicted_block2<0.5]=0\n",
    "\n",
    "print(y_predicted_block2)\n",
    "print(\"Regression block 2 Coefficients: \", regr_block2.coef_)\n",
    "print(\"Regression block 2 Intercept: \", regr_block2.intercept_)\n",
    "print(\"Variance explained, calculated by SK-learn method: %.2f\" % regr_block2.score(data_test_b2, target_test_b2))\n",
    "print(\"Mean Squared Error: \" + str(np.mean((y_predicted_block2 - y_true_block2) ** 2)))\n",
    "# Another way to calculate MSE in SK_slean\n",
    "print(\"Mean Squared Error, version 2: \" + str(mean_squared_error(y_true_block2, y_predicted_block2, multioutput='raw_values')))\n",
    "\n",
    "#10 cross-folds\n",
    "scores = cross_val_score(regr_block2, data_b2, target_b2, cv=10)\n",
    "print(\"10 fold scores are\")\n",
    "print(scores)\n",
    "print(\"Average Accuracy = %.2f\" % np.mean(scores))\n",
    "\n",
    "#calculate the percentage of benign\n",
    "print(y_data)\n",
    "\n",
    "benign_count=len(dataframe_all[dataframe_all['Diagnosis']==\"B\"])\n",
    "print(benign_count)\n",
    "print(\"the percentage of benign in the dataset is:\"+str(benign_count/len(dataframe_all)))\n",
    "\n",
    "#try logistic regression\n",
    "classifier = LogisticRegression(solver=\"lbfgs\",max_iter=1000)\n",
    "# Fit the model\n",
    "classifier.fit(data_b2, target_b2)\n",
    "\n",
    "\n",
    "# Cross Validation\n",
    "scores = cross_val_score(classifier, data_b2, target_b2, cv=10)\n",
    "print(\"10 fold scores are\")\n",
    "print(scores)\n",
    "print(\"Average Accuracy = %.2f\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________Continuous_Variables__________\n",
      "gre mean = 618.57\n",
      "gre median = 620.00\n",
      "gpa mean = 3.49\n",
      "gpa median = 3.54\n",
      "\n",
      "\n",
      "gre mean = 573.58\n",
      "gre median = 580.00\n",
      "gpa mean = 3.35\n",
      "gpa median = 3.34\n",
      "________Categorical_Variables__________\n",
      "the mode for prestige of the admitted students is: 2.0\n",
      "\n",
      "\n",
      "the mode for prestige of the non_admitted students is: 2.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import e\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "plt.rc(\"font\", size=14)\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "dataframe_all = pd.read_csv(\"admissions.csv\")\n",
    "dataframe_all.dropna(inplace=True)\n",
    "# Split into survivors and non-survivors\n",
    "admitted = dataframe_all[dataframe_all['admit'] == 1]\n",
    "non_admitted = dataframe_all[dataframe_all['admit'] == 0]\n",
    "\n",
    "# Continuous categories\n",
    "continuous_variables = [\"gre\", \"gpa\" ]\n",
    "print (\"________Continuous_Variables__________\")\n",
    "\n",
    "# Print out means for admitted students\n",
    "for variable in continuous_variables:\n",
    "    print(\"%s mean = %.2f\" % (variable, admitted[variable].mean()))\n",
    "    print(\"%s median = %.2f\" % (variable, admitted[variable].median()))\n",
    "print(\"\\n\")\n",
    "# Print out means for non_admitted students\n",
    "for variable in continuous_variables:\n",
    "    print(\"%s mean = %.2f\" % (variable, non_admitted[variable].mean()))\n",
    "    print(\"%s median = %.2f\" % (variable, non_admitted[variable].median()))\n",
    "# Categorical categories\n",
    "categorical_variables = [\"prestige\"]\n",
    "\n",
    "print (\"________Categorical_Variables__________\")\n",
    "\n",
    "# print out mode for admitted students:\n",
    "for var in categorical_variables:\n",
    "    print(\"the mode for \"+var+\" of the admitted students is: \"+str(admitted[var].mode().iloc[0]))\n",
    "    \n",
    "print(\"\\n\")\n",
    "#print out mode for non-admitted students:\n",
    "for var in categorical_variables:\n",
    "    print(\"the mode for \"+var+\" of the non_admitted students is: \"+str(non_admitted[var].mode().iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 1. 4. 2.]\n",
      "gre mean = 611.80\n",
      "gre median = 600.00\n",
      "gpa mean = 3.45\n",
      "gpa median = 3.53\n",
      "\n",
      "\n",
      "gre mean = 596.62\n",
      "gre median = 600.00\n",
      "gpa mean = 3.37\n",
      "gpa median = 3.38\n",
      "\n",
      "\n",
      "gre mean = 574.88\n",
      "gre median = 580.00\n",
      "gpa mean = 3.43\n",
      "gpa median = 3.43\n",
      "\n",
      "\n",
      "gre mean = 570.15\n",
      "gre median = 560.00\n",
      "gpa mean = 3.32\n",
      "gpa median = 3.33\n",
      "\n",
      "\n",
      "The observed probability of admission for rank 1 is: 1.1785714285714286\n",
      "The observed probability of admission for rank 1 is: 0.5578947368421052\n",
      "The observed probability of admission for rank 1 is: 0.3010752688172043\n",
      "The observed probability of admission for rank 1 is: 0.21818181818181817\n"
     ]
    }
   ],
   "source": [
    "print(dataframe_all['prestige'].unique())\n",
    "P_1 = dataframe_all[dataframe_all['prestige'] == 1]\n",
    "P_2 = dataframe_all[dataframe_all['prestige'] == 2]\n",
    "P_3 = dataframe_all[dataframe_all['prestige'] == 3]\n",
    "P_4 = dataframe_all[dataframe_all['prestige'] == 4]\n",
    "\n",
    "for variable in continuous_variables:\n",
    "    print(\"%s mean = %.2f\" % (variable, P_1[variable].mean()))\n",
    "    print(\"%s median = %.2f\" % (variable, P_1[variable].median()))\n",
    "print(\"\\n\")\n",
    "for variable in continuous_variables:\n",
    "    print(\"%s mean = %.2f\" % (variable, P_2[variable].mean()))\n",
    "    print(\"%s median = %.2f\" % (variable, P_2[variable].median()))\n",
    "print(\"\\n\")\n",
    "for variable in continuous_variables:\n",
    "    print(\"%s mean = %.2f\" % (variable, P_3[variable].mean()))\n",
    "    print(\"%s median = %.2f\" % (variable, P_3[variable].median()))\n",
    "print(\"\\n\")\n",
    "for variable in continuous_variables:\n",
    "    print(\"%s mean = %.2f\" % (variable, P_4[variable].mean()))\n",
    "    print(\"%s median = %.2f\" % (variable, P_4[variable].median()))\n",
    "print(\"\\n\")\n",
    "\n",
    "#calculate the observed probability of admission\n",
    "P1_admission=len(P_1[P_1['admit']==1])\n",
    "P1_non_admission=len(P_1[P_1['admit']==0])\n",
    "P1_prob=P1_admission/P1_non_admission\n",
    "P2_admission=len(P_2[P_2['admit']==1])\n",
    "P2_non_admission=len(P_2[P_2['admit']==0])\n",
    "P2_prob=P2_admission/P2_non_admission\n",
    "P3_admission=len(P_3[P_3['admit']==1])\n",
    "P3_non_admission=len(P_3[P_3['admit']==0])\n",
    "P3_prob=P3_admission/P3_non_admission\n",
    "P4_admission=len(P_4[P_4['admit']==1])\n",
    "P4_non_admission=len(P_4[P_4['admit']==0])\n",
    "P4_prob=P4_admission/P4_non_admission\n",
    "print(\"The observed probability of admission for rank 1 is: \"+ str(P1_prob))\n",
    "print(\"The observed probability of admission for rank 1 is: \"+ str(P2_prob))\n",
    "print(\"The observed probability of admission for rank 1 is: \"+ str(P3_prob))\n",
    "print(\"The observed probability of admission for rank 1 is: \"+ str(P4_prob))\n",
    "#print(P1_admission)\n",
    "#print(P1_non_admission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold scores are\n",
      "[0.80487805 0.625      0.725      0.725      0.675      0.7\n",
      " 0.71794872 0.69230769 0.74358974 0.66666667]\n",
      "Average Accuracy = 0.71\n",
      "AIC is: 259.923920728119\n",
      "BIC is: 14.263785203753981\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "target_column = ['admit']\n",
    "# Data columns\n",
    "data_columns = [\"gre\",\"gpa\",\"prestige\"]\n",
    "\n",
    "shuffle(dataframe_all)\n",
    "\n",
    "# Split data into data and target\n",
    "admission_final_data = dataframe_all[data_columns]\n",
    "admission_final_target = dataframe_all[target_column]\n",
    "\n",
    "# Convert data and target into dicts\n",
    "admission_dict_data = admission_final_data.T.to_dict('dict').values()\n",
    "admission_dict_target = admission_final_target.T.to_dict('dict').values()\n",
    "\n",
    "# Make new vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform data and target\n",
    "data = vectorizer.fit_transform(admission_dict_data)\n",
    "target = vectorizer.fit_transform(admission_dict_target)\n",
    "# Reshape target\n",
    "target = target.reshape((dataframe_all.shape[0], ))\n",
    "# Create new logistic regression instance\n",
    "classifier = LogisticRegression(solver=\"lbfgs\",max_iter=1000)\n",
    "# Fit the model\n",
    "classifier.fit(data, target)\n",
    "\n",
    "\n",
    "# Cross Validation\n",
    "scores = cross_val_score(classifier, data, target, cv=10)\n",
    "print(\"10 fold scores are\")\n",
    "print(scores)\n",
    "print(\"Average Accuracy = %.2f\" % np.mean(scores))\n",
    "\n",
    "probability_estimation_matrix=classifier.predict_proba(data)\n",
    "#print(classifier.predict_proba(data))\n",
    "result_matrix=probability_estimation_matrix.dot([0,1])\n",
    "#print(result_matrix)\n",
    "sum=np.sum(result_matrix,axis=0)\n",
    "#print(sum)\n",
    "#row_sum=np.sum(probability_estimation_matrix,axis=1).reshape(-1,1)\n",
    "#print(row_sum)\n",
    "k = len(data_columns)+1\n",
    "n = dataframe_all.shape[0]\n",
    "#print(k)\n",
    "#print(n)\n",
    "AIC=2*k+2*sum\n",
    "BIC=np.log(n**k/sum**2)\n",
    "print(\"AIC is: \"+str(AIC))\n",
    "print(\"BIC is: \"+ str(BIC))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold scores are\n",
      "[0.82926829 0.6        0.775      0.775      0.725      0.7\n",
      " 0.71794872 0.66666667 0.74358974 0.66666667]\n",
      "Average Accuracy = 0.72\n",
      "AIC is: 259.9999636967994\n",
      "BIC is: 14.263181596966467\n",
      "the probability of admission is:0.46494464944649444\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "target_column = ['admit']\n",
    "# Data columns\n",
    "data_columns = [\"gpa\",\"prestige\"]\n",
    "#\"gre\",\"gpa\",\"prestige\"\n",
    "shuffle(dataframe_all)\n",
    "\n",
    "# Split data into data and target\n",
    "admission_final_data = dataframe_all[data_columns]\n",
    "admission_final_target = dataframe_all[target_column]\n",
    "\n",
    "# Convert data and target into dicts\n",
    "admission_dict_data = admission_final_data.T.to_dict('dict').values()\n",
    "admission_dict_target = admission_final_target.T.to_dict('dict').values()\n",
    "\n",
    "# Make new vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform data and target\n",
    "data = vectorizer.fit_transform(admission_dict_data)\n",
    "target = vectorizer.fit_transform(admission_dict_target)\n",
    "# Reshape target\n",
    "target = target.reshape((dataframe_all.shape[0], ))\n",
    "# Create new logistic regression instance\n",
    "classifier2 = LogisticRegression(solver=\"lbfgs\",max_iter=1000)\n",
    "# Fit the model\n",
    "classifier2.fit(data, target)\n",
    "\n",
    "\n",
    "# Cross Validation\n",
    "scores = cross_val_score(classifier2, data, target, cv=10)\n",
    "print(\"10 fold scores are\")\n",
    "print(scores)\n",
    "print(\"Average Accuracy = %.2f\" % np.mean(scores))\n",
    "\n",
    "probability_estimation_matrix=classifier2.predict_proba(data)\n",
    "#print(classifier2.predict_proba(data))\n",
    "result_matrix=probability_estimation_matrix.dot([0,1])\n",
    "#print(result_matrix)\n",
    "sum=np.sum(result_matrix,axis=0)\n",
    "#print(sum)\n",
    "#row_sum=np.sum(probability_estimation_matrix,axis=1).reshape(-1,1)\n",
    "#print(row_sum)\n",
    "#k = len(data_columns)+1\n",
    "#n = dataframe_all.shape[0]\n",
    "#print(k)\n",
    "#print(n)\n",
    "AIC=2*k+2*sum\n",
    "BIC=np.log(n**k/sum**2)\n",
    "print(\"AIC is: \"+str(AIC))\n",
    "print(\"BIC is: \"+ str(BIC))\n",
    "\n",
    "admission_len=len(dataframe_all[dataframe_all['admit']==1])\n",
    "non_admission_len=len(dataframe_all[dataframe_all['admit']==0])\n",
    "ratio=admission_len/non_admission_len\n",
    "print(\"the probability of admission is:\"+str(ratio))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Afghanistan', '5', '1', '648', '16', '10', '2', '0', '3', '5.1', '1.1',\n",
      "       '1.2', '0.1', '1.3', '1.4', '1.5', '0.2', 'green', '0.3', '0.4', '0.5',\n",
      "       '0.6', '1.6', '0.7', '0.8', '1.7', '0.9', '0.10', 'black', 'green.1'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-288-cfa09510920e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Convert data and target into dicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdict_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mdict_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mto_dict\u001b[0;34m(self, into)\u001b[0m\n\u001b[1;32m   1492\u001b[0m         \"\"\"\n\u001b[1;32m   1493\u001b[0m         \u001b[0;31m# GH16122\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m         \u001b[0minto_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minto_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mstandardize_mapping\u001b[0;34m(into)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0minto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unsupported type: {into}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minto\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         raise TypeError(\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\"\"\"cols=['name','landmass','zone','area','population','language','religion','bars','stripes','colors','red',\n",
    "     'green','blue','gold','white','black','orange','mainhue','circles','crosses','saltires','quarters','sunstars',\n",
    "     'crescent','triangle','icon','animate','text','topleft','botright']\"\"\"\n",
    "\n",
    "df=pd.read_csv(\"flag.data\",index_col=False)\n",
    "shuffle(df)\n",
    "df.dropna(inplace=True)\n",
    "print(df.columns)\n",
    "\n",
    "target=df.iloc[:,6]\n",
    "#print(target)\n",
    "data=df.iloc[:,1].astype(\"float\")\n",
    "\n",
    "\n",
    "# Convert data and target into dicts\n",
    "dict_data = data.T.to_dict('dict').values()\n",
    "dict_target = target.T.to_dict('dict').values()\n",
    "\n",
    "# Make new vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform data and target\n",
    "data = vectorizer.fit_transform(dict_data)\n",
    "target = vectorizer.fit_transform(dict_target)\n",
    "# Reshape target\n",
    "target = target.reshape((df.shape[0], ))\n",
    "# Create new logistic regression instance\n",
    "classifier = LogisticRegression(solver=\"lbfgs\",max_iter=1000)\n",
    "# Fit the model\n",
    "classifier.fit(data, target)\n",
    "\n",
    "\n",
    "# Cross Validation\n",
    "scores = cross_val_score(classifier, data, target, cv=10)\n",
    "print(\"10 fold scores are\")\n",
    "print(scores)\n",
    "print(\"Average Accuracy = %.2f\" % np.mean(scores))\n",
    "\n",
    "probability_estimation_matrix=classifier.predict_proba(data)\n",
    "#print(classifier2.predict_proba(data))\n",
    "result_matrix=probability_estimation_matrix.dot([0,1])\n",
    "#print(result_matrix)\n",
    "sum=np.sum(result_matrix,axis=0)\n",
    "#print(sum)\n",
    "#row_sum=np.sum(probability_estimation_matrix,axis=1).reshape(-1,1)\n",
    "#print(row_sum)\n",
    "#k = len(data_columns)+1\n",
    "#n = dataframe_all.shape[0]\n",
    "#print(k)\n",
    "#print(n)\n",
    "AIC=2*k+2*sum\n",
    "BIC=np.log(n**k/sum**2)\n",
    "print(\"AIC is: \"+str(AIC))\n",
    "print(\"BIC is: \"+ str(BIC))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
      "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
      "      dtype='object')\n",
      "(303, 1)\n",
      "10 fold scores are\n",
      "[0.87096774 0.80645161 0.83870968 0.87096774 0.90322581 0.73333333\n",
      " 0.83333333 0.86666667 0.68965517 0.72413793]\n",
      "Average Accuracy = 0.81\n",
      "AIC is: 335.9976268539999\n",
      "BIC is: 7.701908954324973\n",
      "[[-0.00323396 -0.72870407 -0.00392638  0.81238492 -0.83350705  0.00164971\n",
      "  -0.53986297  0.41405014 -1.43608701  0.49701363 -0.8414262   0.02319352\n",
      "  -0.01840942]]\n",
      "13\n",
      "(303, 1)\n",
      "10 fold scores are\n",
      "[0.93548387 0.87096774 0.90322581 0.87096774 0.87096774 0.76666667\n",
      " 0.83333333 0.93333333 0.75862069 0.65517241]\n",
      "Average Accuracy = 0.84\n",
      "AIC is: 336.0036636752701\n",
      "BIC is: 7.701872367600934\n",
      "[[-0.73186402  0.7980464  -0.95463918 -0.60733082  0.46638956 -1.36670591\n",
      "   0.44611106  0.0199979 ]]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "df=pd.read_csv(\"heart.csv\",index_col=False)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "target_col=['target']\n",
    "data_cols=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach','exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
    "from sklearn.utils import shuffle\n",
    "shuffle(df)\n",
    "\n",
    "# Split data into data and target\n",
    "df_data = df[data_cols]\n",
    "df_target = df[target_col]\n",
    "\n",
    "print(df_target.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Convert data and target into dicts\n",
    "df_dict_data = df_data.T.to_dict('dict').values()\n",
    "df_dict_target = df_target.T.to_dict('dict').values()\n",
    "\n",
    "# Make new vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform data and target\n",
    "data = vectorizer.fit_transform(df_dict_data)\n",
    "target = vectorizer.fit_transform(df_dict_target)\n",
    "# Reshape target\n",
    "target = target.reshape((df.shape[0], ))\n",
    "# Create new logistic regression instance\n",
    "classifier = LogisticRegression(solver=\"lbfgs\",max_iter=1000)\n",
    "# Fit the model\n",
    "classifier.fit(data, target)\n",
    "\n",
    "\n",
    "# Cross Validation\n",
    "scores = cross_val_score(classifier, data, target, cv=10)\n",
    "print(\"10 fold scores are\")\n",
    "print(scores)\n",
    "print(\"Average Accuracy = %.2f\" % np.mean(scores))\n",
    "\n",
    "#AIC/BIC\n",
    "probability_estimation_matrix=classifier.predict_proba(data)\n",
    "#print(classifier2.predict_proba(data))\n",
    "result_matrix=probability_estimation_matrix.dot([0,1])\n",
    "#print(result_matrix)\n",
    "sum=np.sum(result_matrix,axis=0)\n",
    "\n",
    "AIC=2*k+2*sum\n",
    "BIC=np.log(n**k/sum**2)\n",
    "print(\"AIC is: \"+str(AIC))\n",
    "print(\"BIC is: \"+ str(BIC))\n",
    "\n",
    "#model interpretation\n",
    "print(classifier.coef_)\n",
    "print(len(classifier.coef_[0]))\n",
    "\n",
    "\n",
    "\n",
    "#Best-fitting model\n",
    "target_col=['target']\n",
    "data_cols=['sex', 'cp',   'restecg', 'thalach','exang', 'oldpeak', 'slope', 'ca']\n",
    "from sklearn.utils import shuffle\n",
    "shuffle(df)\n",
    "\n",
    "# Split data into data and target\n",
    "df_data = df[data_cols]\n",
    "df_target = df[target_col]\n",
    "\n",
    "print(df_target.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Convert data and target into dicts\n",
    "df_dict_data = df_data.T.to_dict('dict').values()\n",
    "df_dict_target = df_target.T.to_dict('dict').values()\n",
    "\n",
    "# Make new vectorizer\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "# Transform data and target\n",
    "data = vectorizer.fit_transform(df_dict_data)\n",
    "target = vectorizer.fit_transform(df_dict_target)\n",
    "# Reshape target\n",
    "target = target.reshape((df.shape[0], ))\n",
    "# Create new logistic regression instance\n",
    "classifier = LogisticRegression(solver=\"lbfgs\",max_iter=1000)\n",
    "# Fit the model\n",
    "classifier.fit(data, target)\n",
    "\n",
    "\n",
    "# Cross Validation\n",
    "scores = cross_val_score(classifier, data, target, cv=10)\n",
    "print(\"10 fold scores are\")\n",
    "print(scores)\n",
    "print(\"Average Accuracy = %.2f\" % np.mean(scores))\n",
    "\n",
    "#AIC/BIC\n",
    "probability_estimation_matrix=classifier.predict_proba(data)\n",
    "#print(classifier2.predict_proba(data))\n",
    "result_matrix=probability_estimation_matrix.dot([0,1])\n",
    "#print(result_matrix)\n",
    "sum=np.sum(result_matrix,axis=0)\n",
    "\n",
    "AIC=2*k+2*sum\n",
    "BIC=np.log(n**k/sum**2)\n",
    "print(\"AIC is: \"+str(AIC))\n",
    "print(\"BIC is: \"+ str(BIC))\n",
    "\n",
    "#model interpretation\n",
    "print(classifier.coef_)\n",
    "print(len(classifier.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
